{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos una red neuronal con una capa de entrada, una capa de salida con una red y L-1 redes ocultas.\n",
    "\n",
    "# Con m datos de entrenamientos.\n",
    "\n",
    "Para $m$ datos de entrenamiento, las expresión anteriores pueden ser resumidas en las siguientes ecuaciones\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "z_1^{(0)}  &z_1^{(1)} & .&.& .&z_1^{(m)}\\\\\n",
    "z_2^{(0)}  &z_2^{(1)} &. &.&  .&z_2^{(m)}\\\\\n",
    ".          & .        &. & &   &.      \\\\\n",
    ".          & .        &  &. &   &.      \\\\\n",
    ".          & .        &  &  & .&      \\\\\n",
    "z_{n^{[l]}}^{(0)}&z_{n^{[l]}}^{(1)} & . & .& .& z_{n^{[l]}}^{(m)}        \\\\\n",
    "\\end{bmatrix}^{[l]}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & . & .& .& \\theta_{1n^{[l-1]}}\\\\\n",
    "\\theta_{21} & \\theta_{22} & . & .& .& \\theta_{2n^{[l-1]}}\\\\\n",
    ". & .  & . &   & & .\\\\\n",
    ". & .  &   & . & & .\\\\\n",
    ". & .  &   &  & .& .\\\\\n",
    "\\theta_{n^{[l]}1} & \\theta_{n^{[l]}2} & . & .& .& \\theta_{n^{[l]}n^{[l-1]}}\\\\\n",
    "\\end{bmatrix}^{[l]}_{n^{[l]} \\times n^{[l-1]}}\n",
    "\\begin{bmatrix}\n",
    "a_1^{(0)}  &a_1^{(1)} & .&.& .&a_1^{(m)}\\\\\n",
    "a_2^{(0)}  &a_2^{(1)} &. &.&  .&a_2^{(m)}\\\\\n",
    ".          & .        &. & &   &.      \\\\\n",
    ".          & .        &  &. &   &.      \\\\\n",
    ".          & .        &  &  & .&      \\\\\n",
    "a_{n^{[L-1]}}^{(0)}&a_{n^{[L-1]}}^{(1)} & . & .& .& a_{n^{[L-1]}}^{(m)}        \\\\\n",
    "\\end{bmatrix}^{[l-1]} +\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "b_{n^{[l]}}\\\\\n",
    "\\end{bmatrix}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Escrito de una formas mas compacta tenemos que:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "[ \\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ]= \\Theta^{[l]} [\\vec{A}^{[l-1](0)},\\vec{A}^{[l-1](1)},...,\\vec{A}^{[l-1](m)} ]+ \\vec{b}^{[l]}\n",
    "\\end{equation}\n",
    "\n",
    "Aplicando la funcion de activación:\n",
    "\n",
    "\\begin{equation}\n",
    "[\\vec{A}^{[l](0)},\\vec{A}^{[l](1)},...,\\vec{A}^{[l](m)} ]=f([\\vec{Z}^{[l](0)},\\vec{Z}^{[l](1)},...,\\vec{Z}^{[l](m)}  ])\n",
    "\\end{equation}\n",
    "\n",
    "Las dimensiones de las expresiones anteriores, pueden ser resumidas en lo siguiente:\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topología de la red.\n",
    "\n",
    "1. Construir un clase  que permita definir una red neuronal con la topología\n",
    "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal :\n",
    "\n",
    "Topology = [n_x, n_h1, n_h2, n_h3, ...,n_y]\n",
    "\n",
    "En este caso:\n",
    "- $n^{[0]}=n_x$ seran los valores de entradas de la capa de entrada\n",
    "- $n^{[1]}=n_{h1}$ Primera capa oculta de la red neuronal\n",
    "- $n^{[2]}=n_{h2}$ Segunda capa oculta de la red neuronal\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "- $n^{[l]}=n_{hl}$ Segunda capa oculta de la red neuronal\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
    "\n",
    "donde\n",
    "\n",
    "- $\\mathrm{n_x}$: valores de entrada\n",
    "- $\\mathrm{n_{h1}}$: hidden layer 1\n",
    "- $\\mathrm{n_{h2}}$: hidden layer 2\n",
    "- $\\mathrm{n_y}$: last layer\n",
    "\n",
    "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
    "\n",
    "\n",
    "También definir una lista con las funciones de activaciones para cada capa.\n",
    "\n",
    "\n",
    "activation=[None, relu, relu, relu, ...,sigmoid]\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "a. Cada unas de las capas deberá tener los parámetros de inicialización de manera aleatoria:\n",
    "\n",
    "\n",
    "La matriz de parametros para cada capa debera tener:\n",
    "\n",
    "\n",
    "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
    "\n",
    "Lo anteriores parametros deberán estar en el constructor de la clase.\n",
    "\n",
    "\n",
    "b. Construya un metodo llamado output cuya salida serán los valores de Z y A\n",
    "\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
    "\n",
    "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNN:\n",
    "    def __init__(self, act_fun, nlayer_present, nlayer_before):\n",
    "        self.theta = 2 * np.random.random((nlayer_present, nlayer_before)) - 1\n",
    "        self.B = 2 * np.random.random((nlayer_present, 1)) - 1\n",
    "        self.act_fun = act_fun\n",
    "    \n",
    "    def output(self, Z, A):\n",
    "        self.Z = Z\n",
    "        self.A = A\n",
    "\n",
    "def act_function(activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        f = lambda x: 1 / (1 + np.exp(-x))\n",
    "        fp = lambda x: f(x) * (1 - f(x))\n",
    "    elif activation == \"tanh\":\n",
    "        f = np.tanh\n",
    "        fp = lambda x: 1 - np.tanh(x)**2\n",
    "    elif activation == \"relu\":\n",
    "        f = lambda x: np.maximum(0, x)\n",
    "        fp = lambda x: np.where(x > 0, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "    return f, fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construir un generalizacion de la red, en el que entrada el valor inicial\n",
    "y la red neuronal completa arroje la salida y la actualizacion de la red con los parametros deseados:\n",
    "\n",
    "  ```\n",
    "  A, nn = forward_pass(A0, nn_red)\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(A0, nn_red):\n",
    "    A = A0\n",
    "    for layer in nn_red:\n",
    "        Z = np.dot(layer.theta, A) + layer.B\n",
    "        A = layer.act_fun[0](Z)\n",
    "        layer.output(Z, A)\n",
    "    return A, nn_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Encontrar la funcion de coste.\n",
    "\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, AL):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / m\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Construir un codigo que permita realizar el BackwardPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(Y, nn_red, alpha):\n",
    "    m = Y.shape[1]\n",
    "    L = len(nn_red)\n",
    "    dAL = -(np.divide(Y, nn_red[-1].A) - np.divide(1 - Y, 1 - nn_red[-1].A))\n",
    "    \n",
    "    dA = dAL\n",
    "    for l in reversed(range(L)):\n",
    "        layer = nn_red[l]\n",
    "        activation_derivative = layer.act_fun[1](layer.Z)\n",
    "        dZ = dA * activation_derivative\n",
    "        dTheta = (1 / m) * np.dot(dZ, nn_red[l - 1].A.T) if l > 0 else (1 / m) * np.dot(dZ, A0.T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        dA = np.dot(layer.theta.T, dZ)\n",
    "        \n",
    "        layer.theta -= alpha * dTheta\n",
    "        layer.B -= alpha * db\n",
    "    \n",
    "    return nn_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y a continuación se escriben pruebas de todas estas funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo Inicial: 12.942850225892188\n",
      "Costo Actualizado: 7.432876526846854\n",
      "Costo de Prueba: 16.719095325458902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Cargar datos\n",
    "data_train = \"train_catvnoncat.h5\"\n",
    "train_dataset = h5py.File(data_train, \"r\")\n",
    "\n",
    "data_test = \"test_catvnoncat.h5\"\n",
    "test_dataset = h5py.File(data_test, \"r\")\n",
    "\n",
    "# Leer los datos\n",
    "xtrain_classes, xtrain, train_label = (\n",
    "    train_dataset[\"list_classes\"], train_dataset[\"train_set_x\"], np.array(train_dataset[\"train_set_y\"])\n",
    ")\n",
    "\n",
    "test_classes, xtest, test_label = (\n",
    "    test_dataset[\"list_classes\"], test_dataset[\"test_set_x\"], np.array(test_dataset[\"test_set_y\"])\n",
    ")\n",
    "\n",
    "# Redimensionar datos\n",
    "xtrain_ = np.reshape(xtrain, (xtrain.shape[0], -1)).T / 255\n",
    "xtest_ = np.reshape(xtest, (xtest.shape[0], -1)).T / 255\n",
    "\n",
    "# Definir topología y activaciones\n",
    "topology = [xtrain_.shape[0], 10, 5, 1]\n",
    "activations = [None, act_function(\"relu\"), act_function(\"relu\"), act_function(\"sigmoid\")]\n",
    "\n",
    "# Inicializar red neuronal\n",
    "nn_red = [LayerNN(activations[i], topology[i], topology[i-1]) for i in range(1, len(topology))]\n",
    "\n",
    "# Entrenar red\n",
    "A0 = xtrain_\n",
    "Y = train_label.reshape(1, -1)\n",
    "\n",
    "\n",
    "# forward pass\n",
    "AL, nn_red = forward_pass(A0, nn_red)\n",
    "\n",
    "# Calcular costo\n",
    "cost = compute_cost(Y, AL)\n",
    "print(\"Costo Inicial:\", cost)\n",
    "\n",
    "# Paso hacia atrás\n",
    "alpha = 0.01\n",
    "nn_red = backward_pass(Y, nn_red, alpha)\n",
    "\n",
    "# forward pass después de actualizar\n",
    "AL, nn_red = forward_pass(A0, nn_red)\n",
    "new_cost = compute_cost(Y, AL)\n",
    "print(\"Costo Actualizado:\", new_cost)\n",
    "\n",
    "# Evaluar en conjunto de prueba\n",
    "A0_test = xtest_\n",
    "test_Y = test_label.reshape(1, -1)\n",
    "AL_test, _ = forward_pass(A0_test, nn_red)\n",
    "test_cost = compute_cost(test_Y, AL_test)\n",
    "print(\"Costo de Prueba:\", test_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
