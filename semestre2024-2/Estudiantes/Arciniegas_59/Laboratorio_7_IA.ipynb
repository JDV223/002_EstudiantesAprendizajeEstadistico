{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 7: Red neuronal de clasificación binaria\n",
        "\n",
        "Estudiante: Alejandra Arciniegas Marin, C.C 1000662159"
      ],
      "metadata": {
        "id": "YXwLUIu3IW9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TexePtuhIWuB"
      },
      "outputs": [],
      "source": [
        "#@title Importar librerías\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definición de la clase para definir la red neuronal:\n",
        "\n",
        "class Red_neuronal():\n",
        "    '''\n",
        "    Clase que representa una red neuronal para clasificación binaria.\n",
        "\n",
        "    Parámetros del constructor:\n",
        "    ----------\n",
        "    L: int\n",
        "        Número de capas de la red.\n",
        "    topology : list of int\n",
        "        Lista con el número de neuronas que tendrá cada capa.\n",
        "    activation : list of str\n",
        "        Funciones de activación a utilizar en cada capa.\n",
        "    N_epochs : int\n",
        "        Número de épocas para entrenar el algoritmo.\n",
        "    N_train : int\n",
        "        Número de datos de entrenamiento.\n",
        "    N_test : int\n",
        "        Número de datos de prueba.\n",
        "    Xtrain : numpy.ndarray\n",
        "        Arreglo con los datos de entrenamiento. Las filas deben ser las características y las columnas los datos.\n",
        "    Ytrain : numpy.ndarray\n",
        "        Arreglo con las etiquetas de los datos de entrenamiento.\n",
        "    Xtest : numpy.ndarray\n",
        "        Arreglo con los datos de prueba. Las filas deben ser las características y las columnas los datos.\n",
        "    Ytest : numpy.ndarray\n",
        "        Arreglo con las etiquetas de los datos de prueba.\n",
        "    alpha : float\n",
        "        Tasa de aprendizaje del gradiente descendente.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, L, topology, activation, N_epochs, N_train, N_test, Xtrain, Ytrain, Xtest, Ytest, alpha):\n",
        "        self.L = L\n",
        "        self.topology = topology\n",
        "        self.activation = activation\n",
        "        self.N_epochs = N_epochs\n",
        "        self.m = N_train\n",
        "        self.x = Xtrain\n",
        "        self.y = Ytrain\n",
        "        self.N_test = N_test\n",
        "        self.Xtest = Xtest\n",
        "        self.Ytest = Ytest\n",
        "        self.alpha = alpha\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Inicialización de pesos y biases\n",
        "        self.Initialization()\n",
        "\n",
        "    def activation_function(self, name, x):\n",
        "        '''\n",
        "        Calcula la función de activación para una capa.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        name : str\n",
        "            El nombre de la función de activación ('sigmoid', 'relu', 'tanh').\n",
        "        x : numpy.ndarray\n",
        "            El valor de entrada.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        numpy.ndarray\n",
        "            El valor de la función de activación aplicada a x.\n",
        "        '''\n",
        "        if name == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif name == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        elif name == 'tanh':\n",
        "            return np.tanh(x)\n",
        "\n",
        "    def der_activation_function(self, name, x):\n",
        "        '''\n",
        "        Calcula la derivada de la función de activación para una capa.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        name : str\n",
        "            El nombre de la función de activación ('sigmoid', 'relu', 'tanh').\n",
        "        x : numpy.ndarray\n",
        "            El valor de entrada.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        numpy.ndarray\n",
        "            El valor de la derivada de la función de activación aplicada a x.\n",
        "        '''\n",
        "        if name == 'sigmoid':\n",
        "            sigmoid = self.activation_function('sigmoid', x)\n",
        "            return sigmoid * (1 - sigmoid)\n",
        "        elif name == 'relu':\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        elif name == 'tanh':\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def Forward_Pass(self, w_l, a_l_1, b_l, f):\n",
        "        '''\n",
        "        Realiza el paso hacia adelante de una capa en la red.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        w_l : numpy.ndarray\n",
        "            Los pesos de la capa actual.\n",
        "        a_l_1 : numpy.ndarray\n",
        "            La activación de la capa anterior.\n",
        "        b_l : numpy.ndarray\n",
        "            Los sesgos de la capa actual.\n",
        "        f : str\n",
        "            La función de activación de la capa actual.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        a_l : numpy.ndarray\n",
        "            La activación de la capa actual.\n",
        "        z_l : numpy.ndarray\n",
        "            El valor antes de la función de activación.\n",
        "        '''\n",
        "        z_l = np.dot(w_l, a_l_1) + b_l\n",
        "        a_l = self.activation_function(f, z_l)\n",
        "        return a_l, z_l\n",
        "\n",
        "    def Cost_func(self, a_l):\n",
        "        '''\n",
        "        Calcula la función de costo.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        a_l : numpy.ndarray\n",
        "            La activación de la última capa.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        float\n",
        "            El valor de la función de costo.\n",
        "        '''\n",
        "        return (-1 / self.m) * np.sum(self.y * np.log(a_l) + (1 - self.y) * np.log(1 - a_l))\n",
        "\n",
        "    def Backward_Pass_out(self, a_L, a_L_1, W_L, b_L, Y):\n",
        "        '''\n",
        "        Realiza el paso hacia atrás para la última capa en la red.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        a_L : numpy.ndarray\n",
        "            La activación de la última capa.\n",
        "        a_L_1 : numpy.ndarray\n",
        "            La activación de la penúltima capa.\n",
        "        W_L : numpy.ndarray\n",
        "            Los pesos de la última capa.\n",
        "        b_L : numpy.ndarray\n",
        "            Los sesgos de la última capa.\n",
        "        Y : numpy.ndarray\n",
        "            Las etiquetas de los datos.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        W_L : numpy.ndarray\n",
        "            Los pesos actualizados de la última capa.\n",
        "        b_L : numpy.ndarray\n",
        "            Los sesgos actualizados de la última capa.\n",
        "        '''\n",
        "        delta_L = a_L - Y\n",
        "        dJdW = (1 / self.m) * np.dot(delta_L, a_L_1.T)\n",
        "        dJdb = (1 / self.m) * np.sum(delta_L, axis=1, keepdims=True)\n",
        "\n",
        "        W_L -= self.alpha * dJdW\n",
        "        b_L -= self.alpha * dJdb\n",
        "\n",
        "        return W_L, b_L\n",
        "\n",
        "    def Backward_Pass(self, name, delta_l1, W_l1, W_l, Z_l, a_l_1, b_l):\n",
        "        '''\n",
        "        Realiza el paso hacia atrás para una capa intermedia en la red.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        name : str\n",
        "            El nombre de la función de activación de la capa actual.\n",
        "        delta_l1 : numpy.ndarray\n",
        "            El error de la capa siguiente.\n",
        "        W_l1 : numpy.ndarray\n",
        "            Los pesos de la capa siguiente.\n",
        "        W_l : numpy.ndarray\n",
        "            Los pesos de la capa actual.\n",
        "        Z_l : numpy.ndarray\n",
        "            El valor antes de la función de activación de la capa actual.\n",
        "        a_l_1 : numpy.ndarray\n",
        "            La activación de la capa anterior.\n",
        "        b_l : numpy.ndarray\n",
        "            Los sesgos de la capa actual.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        W_l : numpy.ndarray\n",
        "            Los pesos actualizados de la capa actual.\n",
        "        b_l : numpy.ndarray\n",
        "            Los sesgos actualizados de la capa actual.\n",
        "        delta_l : numpy.ndarray\n",
        "            El error de la capa actual.\n",
        "        '''\n",
        "        delta_l = np.dot(W_l1.T, delta_l1) * self.der_activation_function(name, Z_l)\n",
        "        dJdW = (1 / self.m) * np.dot(delta_l, a_l_1.T)\n",
        "        dJdb = (1 / self.m) * np.sum(delta_l, axis=1, keepdims=True)\n",
        "\n",
        "        W_l -= self.alpha * dJdW\n",
        "        b_l -= self.alpha * dJdb\n",
        "\n",
        "        return W_l, b_l, delta_l\n",
        "\n",
        "    def Initialization(self):\n",
        "        '''\n",
        "        Inicializa los pesos y sesgos de la red.\n",
        "\n",
        "        Parámetros: No tiene.\n",
        "\n",
        "        Retorno: No tiene.\n",
        "        '''\n",
        "        np.random.seed(1)\n",
        "        self.weights = [np.random.randn(self.topology[i+1], self.topology[i]) * 0.01 for i in range(self.L-1)]\n",
        "        self.biases = [np.zeros((self.topology[i+1], 1)) for i in range(self.L-1)]\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Entrena la red neuronal utilizando propagación hacia adelante y hacia atrás.\n",
        "\n",
        "        Parámetros: No tiene.\n",
        "\n",
        "        Retorno: No tiene.\n",
        "        '''\n",
        "        for epoch in range(self.N_epochs):\n",
        "            a = [self.x]\n",
        "            z = []\n",
        "\n",
        "            # Forward Pass\n",
        "            for i in range(self.L-1):\n",
        "                a_l, z_l = self.Forward_Pass(self.weights[i], a[-1], self.biases[i], self.activation[i])\n",
        "                a.append(a_l)\n",
        "                z.append(z_l)\n",
        "\n",
        "            # Cálculo del costo\n",
        "            cost = self.Cost_func(a[-1])\n",
        "\n",
        "            # Backward Pass\n",
        "            delta_l = a[-1] - self.y\n",
        "            for i in reversed(range(self.L-1)):\n",
        "                if i == self.L-2:\n",
        "                    self.weights[i], self.biases[i] = self.Backward_Pass_out(a[-1], a[-2], self.weights[i], self.biases[i], self.y)\n",
        "                else:\n",
        "                    self.weights[i], self.biases[i], delta_l = self.Backward_Pass(self.activation[i], delta_l, self.weights[i+1], self.weights[i], z[i], a[i], self.biases[i])\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Realiza una predicción utilizando la red neuronal entrenada.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        X : numpy.ndarray\n",
        "            Los datos de entrada para realizar la predicción.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        numpy.ndarray\n",
        "            La predicción de la red neuronal.\n",
        "        '''\n",
        "        a = X  # Inicialización de la activación con los datos de entrada\n",
        "        for i in range(self.L-1):\n",
        "            # Realizar el paso hacia adelante para cada capa\n",
        "            a, _ = self.Forward_Pass(self.weights[i], a, self.biases[i], self.activation[i])\n",
        "        return a  # Retornar la activación de la última capa (predicción)\n",
        "\n",
        "    def score(self, X, Y):\n",
        "        '''\n",
        "        Calcula la exactitud del modelo en los datos proporcionados.\n",
        "\n",
        "        Parámetros:\n",
        "        ----------\n",
        "        X : numpy.ndarray\n",
        "            Los datos de entrada para realizar la predicción.\n",
        "        Y : numpy.ndarray\n",
        "            Las etiquetas reales de los datos de entrada.\n",
        "\n",
        "        Retorno:\n",
        "        ----------\n",
        "        float\n",
        "            La exactitud del modelo, como un valor entre 0 y 1.\n",
        "        '''\n",
        "        predictions = self.predict(X)  # Realizar predicciones en los datos de entrada\n",
        "        predictions = (predictions > 0.5).astype(int)  # Convertir las probabilidades a etiquetas binarias\n",
        "        return np.mean(predictions == Y)  # Calcular y retornar la exactitud de las predicciones\n"
      ],
      "metadata": {
        "id": "MZyDDmYnHV-G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ejecución con el dataset de fotos de gatos\n",
        "\n",
        "# Cargar el dataset\n",
        "data_train = \"train_catvnoncat.h5\"\n",
        "train_dataset = h5py.File(data_train, \"r\")\n",
        "\n",
        "data_test = \"test_catvnoncat.h5\"\n",
        "test_dataset = h5py.File(data_test, \"r\")\n",
        "\n",
        "xtrain_classes, xtrain, train_label =\\\n",
        "train_dataset[\"list_classes\"], train_dataset[\"train_set_x\"], train_dataset[\"train_set_y\"]\n",
        "\n",
        "test_classes, xtest, test_label =\\\n",
        "test_dataset[\"list_classes\"], test_dataset[\"test_set_x\"], test_dataset[\"test_set_y\"]\n",
        "\n",
        "ytrain = np.array(train_label)\n",
        "Xtrain = (np.reshape(xtrain, (xtrain.shape[0], -1)) / 255).T\n",
        "\n",
        "ytest = np.array(test_label)\n",
        "Xtest = (np.reshape(xtest, (xtest.shape[0], -1)) / 255).T\n",
        "\n",
        "# Parámetros de la red\n",
        "L = 4  # Número de capas\n",
        "topology = [Xtrain.shape[0], 10, 10, 1]  # Topología de la red\n",
        "activation = ['relu', 'sigmoid', 'sigmoid', 'sigmoid']  # Funciones de activación\n",
        "N_epochs = 1000  # Número de épocas\n",
        "N_train = Xtrain.shape[1]  # Número de datos de entrenamiento\n",
        "N_test = Xtest.shape[1]  # Número de datos de prueba\n",
        "alpha = 0.01  # Tasa de aprendizaje\n",
        "\n",
        "# Crear y entrenar el modelo\n",
        "model = Red_neuronal(L, topology, activation, N_epochs, N_train, N_test, Xtrain, ytrain, Xtest, ytest, alpha)\n",
        "model.train()\n",
        "\n",
        "# Evaluar el modelo\n",
        "train_score = model.score(Xtrain, ytrain)\n",
        "test_score = model.score(Xtest, ytest)\n",
        "print(f'Train Score: {train_score}')\n",
        "print(f'Test Score: {test_score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNc6K4lvGBuf",
        "outputId": "861a5a04-81ed-46e6-ff13-b306471489dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score: 0.6555023923444976\n",
            "Test Score: 0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poe los puntajes obtenidos, se puede ver que el puntaje de entrenamiento y el de prueba están muy alejados el uno del otro, por tanto hay un overffiting del modelo. De igual forma ambos puntajes son bastante bajitos."
      ],
      "metadata": {
        "id": "0U_tjRdwI0Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ejecución con el dataset del cáncer de mama\n",
        "\n",
        "# Cargar el dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "Y = data.target.reshape(-1, 1)\n",
        "\n",
        "# Dividir en datos de entrenamiento y prueba\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarizar los datos\n",
        "scaler = StandardScaler()\n",
        "Xtrain = scaler.fit_transform(Xtrain)\n",
        "Xtest = scaler.transform(Xtest)\n",
        "\n",
        "# Convertir a las formas requeridas (transponer)\n",
        "Xtrain = Xtrain.T\n",
        "Ytrain = Ytrain.T\n",
        "Xtest = Xtest.T\n",
        "Ytest = Ytest.T\n",
        "\n",
        "# Parámetros de la red\n",
        "L = 4  # Número de capas\n",
        "topology = [Xtrain.shape[0],10,5, 3, 1]  # Topología de la red (ajustada)\n",
        "activation = ['relu', 'sigmoid','sigmoid', 'sigmoid']  # Funciones de activación\n",
        "N_epochs = 1000  # Número de épocas\n",
        "N_train = Xtrain.shape[1]  # Número de datos de entrenamiento\n",
        "N_test = Xtest.shape[1]  # Número de datos de prueba\n",
        "alpha = 0.01  # Tasa de aprendizaje\n",
        "\n",
        "# Creación y entrenamiento del modelo\n",
        "model = Red_neuronal(L, topology, activation, N_epochs, N_train, N_test, Xtrain, Ytrain, Xtest, Ytest, alpha)\n",
        "model.train()\n",
        "\n",
        "# Evaluación del modelo\n",
        "train_score = model.score(Xtrain, Ytrain)\n",
        "test_score = model.score(Xtest, Ytest)\n",
        "print(f'Train Score: {train_score}')\n",
        "print(f'Test Score: {test_score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTxw8KB5B9pU",
        "outputId": "97b74993-65f5-4978-d8d9-d8d75e87e1cb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score: 0.6285714285714286\n",
            "Test Score: 0.6228070175438597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, ambos puntajes son mucho más cercanos el uno del otro, sin embargo siguen siendo bastante lejanos a uno, lo que muestra un mal comportamiento del modelo. En general, el modelo no es muy bueno."
      ],
      "metadata": {
        "id": "DkWYrJ8wJOp2"
      }
    }
  ]
}