{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db670d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Documentación del proyecto final de Aprendizaje Estadístico 2024-2\n",
    "\n",
    "**Integrantes:** Alejandra Arciniegas Marin - Andrés Felipe Riaño Quintanilla - William Esneider Galeano Sierra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617589e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Localización**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab67843",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Entrenamiento y prueba preliminar del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07bc183",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "#Librerías:\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb6968",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Directorio de imágenes y CSVs\n",
    "image_dir = \"./Rets\"\n",
    "fovea_csv_path = \"./IDRiD_Fovea_Center.csv\"\n",
    "od_csv_path = \"./IDRiD_OD_Center.csv\"\n",
    "output_mask_dir = \"./Masks\"\n",
    "os.makedirs(output_mask_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849d1fb",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Parámetros de las máscaras\n",
    "image_size = (224, 224)  # Tamaño redimensionado de las imágenes\n",
    "background_gray = 128  # Color del fondo de la máscara\n",
    "fovea_circle_radius = 10  # Radio del círculo negro para la fóvea\n",
    "od_circle_radius = 10  # Radio del círculo blanco para el disco óptico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a457",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Leer CSVs\n",
    "fovea_data = pd.read_csv(fovea_csv_path)\n",
    "od_data = pd.read_csv(od_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf9431",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Combinar las coordenadas en un DataFrame\n",
    "coordinates_df = pd.merge(fovea_data, od_data, on=\"Image No\", suffixes=(\"_fovea\", \"_od\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59162d2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Función para crear una máscara de segmentación personalizada\n",
    "def create_mask(fovea_coords, od_coords):\n",
    "    \"\"\"\n",
    "    Crea una máscara con dos regiones diferenciadas:\n",
    "    - Un círculo negro para la fóvea\n",
    "    - Un círculo blanco para el disco óptico\n",
    "\n",
    "    Parámetros:\n",
    "    - fovea_coords: coordenadas de la fóvea en el formato (x, y)\n",
    "    - od_coords: coordenadas del disco óptico en el formato (x, y)\n",
    "    \n",
    "    Retorno:\n",
    "    - mask: imagen en escala de grises con la máscara generada\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear un fondo gris uniforme del tamaño de la imagen\n",
    "    # image_size[::-1]: dimensiones de la imagen en formato (ancho, alto)\n",
    "    # dtype=np.uint8: tipo de dato para imágenes de 8 bits (valores entre 0 y 255)\n",
    "    # background_gray: nivel de gris para el fondo\n",
    "    mask = np.ones((*image_size[::-1],), dtype=np.uint8) * background_gray\n",
    "\n",
    "    # Dibujar un círculo negro para la fóvea\n",
    "    # Las coordenadas se ajustan proporcionalmente al tamaño de la imagen original (4288x2848)\n",
    "    # fovea_circle_radius: radio del círculo de la fóvea\n",
    "    # (0): color negro para la región de la fóvea\n",
    "    # -1: relleno completo del círculo\n",
    "    cv2.circle(mask, \n",
    "                (int(fovea_coords[0] * image_size[0] / 4288), \n",
    "                 int(fovea_coords[1] * image_size[1] / 2848)), \n",
    "                fovea_circle_radius, (0), -1)\n",
    "\n",
    "    # Dibujar un círculo blanco para el disco óptico\n",
    "    # od_circle_radius: radio del círculo del disco óptico\n",
    "    # (255): color blanco para la región del disco óptico\n",
    "    cv2.circle(mask, \n",
    "                (int(od_coords[0] * image_size[0] / 4288), \n",
    "                 int(od_coords[1] * image_size[1] / 2848)), \n",
    "                od_circle_radius, (255), -1)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7e1ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Generación de máscaras de segmentación\n",
    "print(\"Generando máscaras...\")\n",
    "\n",
    "# Iterar sobre cada fila del DataFrame que contiene las coordenadas de la fóvea y el disco óptico\n",
    "for _, row in tqdm(coordinates_df.iterrows(), total=len(coordinates_df)):\n",
    "    # Obtener el nombre de la imagen\n",
    "    image_name = row[\"Image No\"]\n",
    "\n",
    "    # Obtener las coordenadas de la fóvea y del disco óptico\n",
    "    fovea_coords = (row[\"X- Coordinate_fovea\"], row[\"Y - Coordinate_fovea\"])\n",
    "    od_coords = (row[\"X- Coordinate_od\"], row[\"Y - Coordinate_od\"])\n",
    "\n",
    "    # Crear la máscara utilizando las coordenadas proporcionadas\n",
    "    mask = create_mask(fovea_coords, od_coords)\n",
    "\n",
    "    # Definir la ruta de salida para guardar la máscara generada\n",
    "    output_path = os.path.join(output_mask_dir, f\"{image_name}_mask.png\")\n",
    "\n",
    "    # Guardar la máscara como imagen PNG en la ubicación definida\n",
    "    cv2.imwrite(output_path, mask)\n",
    "\n",
    "print(f\"Máscaras generadas y guardadas en {output_mask_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffdc12",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Definición de un bloque de atención para la arquitectura Attention U-Net\n",
    "def attention_block(x, g, inter_channel):\n",
    "    \"\"\"\n",
    "    Bloque de atención que ajusta la importancia de las características de una capa en el decoder.\n",
    "    \n",
    "    Parámetros:\n",
    "    - x: tensor de entrada de la capa a la que se aplica la atención.\n",
    "    - g: tensor de entrada guía del decoder.\n",
    "    - inter_channel: número de canales intermedios.\n",
    "\n",
    "    Retorna:\n",
    "    - Tensor resultante después de aplicar la atención.\n",
    "    \"\"\"\n",
    "    # Proyección espacial de la entrada y el tensor guía\n",
    "    theta_x = layers.Conv2D(inter_channel, (1, 1))(x)\n",
    "    phi_g = layers.Conv2D(inter_channel, (1, 1))(g)\n",
    "\n",
    "    # Suma de tensores proyectados y activación\n",
    "    concat = layers.Add()([theta_x, phi_g])\n",
    "    concat = layers.Activation('relu')(concat)\n",
    "\n",
    "    # Psi genera una máscara de atención (probabilidades)\n",
    "    psi = layers.Conv2D(1, (1, 1), activation='sigmoid')(concat)\n",
    "\n",
    "    # Multiplicación para aplicar la atención\n",
    "    return layers.Multiply()([x, psi])\n",
    "\n",
    "\n",
    "# Definición de un bloque convolucional básico\n",
    "def conv_block(x, num_filters):\n",
    "    \"\"\"\n",
    "    Aplica dos convoluciones con Batch Normalization y ReLU.\n",
    "\n",
    "    Parámetros:\n",
    "    - x: tensor de entrada.\n",
    "    - num_filters: número de filtros para las convoluciones.\n",
    "\n",
    "    Retorna:\n",
    "    - Tensor después de aplicar las convoluciones y normalización.\n",
    "    \"\"\"\n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Definición del modelo Attention U-Net\n",
    "def attention_unet(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Crea un modelo Attention U-Net para segmentación de imágenes.\n",
    "\n",
    "    Parámetro:\n",
    "    - input_shape: tamaño de entrada de las imágenes.\n",
    "\n",
    "    Retorna:\n",
    "    - Modelo de Keras Attention U-Net.\n",
    "    \"\"\"\n",
    "    # Entrada del modelo\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    c1 = conv_block(inputs, 64)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = conv_block(p1, 128)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = conv_block(p2, 256)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = conv_block(p3, 512)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = conv_block(p4, 1024)\n",
    "\n",
    "    # --- Decoder ---\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    a6 = attention_block(c4, u6, 512)\n",
    "    u6 = layers.Concatenate()([u6, a6])\n",
    "    c6 = conv_block(u6, 512)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    a7 = attention_block(c3, u7, 256)\n",
    "    u7 = layers.Concatenate()([u7, a7])\n",
    "    c7 = conv_block(u7, 256)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    a8 = attention_block(c2, u8, 128)\n",
    "    u8 = layers.Concatenate()([u8, a8])\n",
    "    c8 = conv_block(u8, 128)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u8)\n",
    "    a9 = attention_block(c1, u9, 64)\n",
    "    u9 = layers.Concatenate()([u9, a9])\n",
    "    c9 = conv_block(u9, 64)\n",
    "\n",
    "    # Capa de salida: un canal binario para segmentación\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    # Definición del modelo final\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d004a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Definición de la métrica Dice Coefficient\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente de Dice, una métrica de similitud utilizada \n",
    "    comúnmente para evaluar la calidad de segmentaciones en imágenes.\n",
    "\n",
    "    Parámetros:\n",
    "    - y_true: Tensor de las máscaras reales.\n",
    "    - y_pred: Tensor de las máscaras predichas.\n",
    "\n",
    "    Retorna:\n",
    "    - Valor del coeficiente de Dice, que varía entre 0 y 1:\n",
    "      * 1 indica una superposición perfecta.\n",
    "      * 0 indica ninguna superposición.\n",
    "    \"\"\"\n",
    "    # Aplanar los tensores de las máscaras (convertirlos a vectores)\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Calcular la intersección entre la máscara real y la predicha\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "\n",
    "    # Fórmula del coeficiente de Dice con suavizado para evitar divisiones por cero\n",
    "    return (2. * intersection + 1) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebff5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "def iou_for_value(y_true, y_pred, value, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calcula la métrica Intersection over Union (IoU) para un valor específico de interés \n",
    "    (por ejemplo, fóvea o disco óptico) en segmentación de imágenes.\n",
    "\n",
    "    Parámetros:\n",
    "    - y_true: Tensor de las máscaras reales.\n",
    "    - y_pred: Tensor de las máscaras predichas.\n",
    "    - value: Valor de interés en la máscara (0.0 para la fóvea, 1.0 para el disco óptico).\n",
    "    - threshold: Umbral para considerar un píxel como parte de la región de interés.\n",
    "\n",
    "    Retorna:\n",
    "    - IoU: Intersection over Union para la región definida por el valor.\n",
    "    \"\"\"\n",
    "    # Crear máscaras booleanas según el valor objetivo y el umbral\n",
    "    true_mask = tf.math.abs(y_true - value) < threshold\n",
    "    pred_mask = tf.math.abs(y_pred - value) < threshold\n",
    "\n",
    "    # Calcular la intersección y la unión de las máscaras\n",
    "    intersection = tf.reduce_sum(tf.cast(tf.logical_and(true_mask, pred_mask), tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(tf.logical_or(true_mask, pred_mask), tf.float32))\n",
    "\n",
    "    # Devolver el resultado del IoU, evitando divisiones por cero\n",
    "    return intersection / (union + tf.keras.backend.epsilon())\n",
    "\n",
    "# Crear funciones parciales para valores específicos de interés\n",
    "iou_for_fovea = partial(iou_for_value, value=0.0)  # IoU para la fóvea (valor 0.0)\n",
    "iou_for_od = partial(iou_for_value, value=1.0)  # IoU para el disco óptico (valor 1.0)\n",
    "\n",
    "# Ajustar los nombres para que se puedan registrar como métricas en TensorFlow\n",
    "iou_for_fovea.__name__ = \"iou_fovea\"\n",
    "iou_for_od.__name__ = \"iou_od\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc2ad4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import logical_and\n",
    "\n",
    "def background_distance_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Métrica personalizada para evaluar la distancia del fondo a un valor gris ideal (0.5).\n",
    "    Penaliza predicciones que se alejen de este valor, lo que ayuda a mantener el fondo en tonos intermedios\n",
    "    y mejorar la segmentación de regiones relevantes (disco óptico y fóvea).\n",
    "\n",
    "    Parámetros:\n",
    "    - y_true: Tensor con las máscaras reales.\n",
    "    - y_pred: Tensor con las máscaras predichas.\n",
    "\n",
    "    Retorna:\n",
    "    - Promedio de la distancia del fondo al valor gris ideal (0.5).\n",
    "    \"\"\"\n",
    "    # Valor ideal para el fondo (gris medio)\n",
    "    ideal_background_value = 0.5\n",
    "\n",
    "    # Máscara para seleccionar solo el fondo (excluir disco óptico y fóvea)\n",
    "    mask_background = K.cast(logical_and(y_true > 0.3, y_true < 0.7), K.floatx())\n",
    "\n",
    "    # Calcular la distancia del fondo al gris ideal\n",
    "    distance = K.abs(y_pred - ideal_background_value) * mask_background\n",
    "\n",
    "    # Promediar las distancias en la región de fondo seleccionada\n",
    "    return K.mean(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d776ba0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Crear y compilar el modelo\n",
    "model = attention_unet(input_shape=(224, 224, 3))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy',dice_coefficient, iou_for_fovea, iou_for_od,background_distance_metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4e886",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Cargar imágenes y máscaras\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "mask_filenames = sorted([f for f in os.listdir(output_mask_dir) if f.endswith(\".png\")])\n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85ae2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "print(\"Cargando imágenes y máscaras...\")\n",
    "\n",
    "# Iterar sobre las listas de archivos de imágenes y máscaras\n",
    "for img_file, mask_file in tqdm(zip(image_filenames, mask_filenames), total=len(image_filenames)):\n",
    "    # Construir las rutas completas de los archivos de imagen y máscara\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    mask_path = os.path.join(output_mask_dir, mask_file)\n",
    "\n",
    "    # Leer la imagen desde el archivo\n",
    "    img = cv2.imread(img_path)\n",
    "    # Redimensionar la imagen al tamaño especificado para ajustarla a la memoria y al modelo\n",
    "    img = cv2.resize(img, image_size)\n",
    "\n",
    "    # Leer la máscara en escala de grises\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Redimensionar la máscara al tamaño especificado\n",
    "    mask = cv2.resize(mask, image_size)\n",
    "    # Expandir la dimensión de la máscara para convertirla en (altura, ancho, 1)\n",
    "    # Normalizar los valores de la máscara a [0, 1]\n",
    "    mask = np.expand_dims(mask, axis=-1) / 255.0\n",
    "\n",
    "    # Añadir la imagen y la máscara a las listas\n",
    "    X.append(img)\n",
    "    y.append(mask)\n",
    "\n",
    "# Convertir las listas a arreglos de NumPy\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054a1f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# División en conjunto de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Callback para mostrar imágenes durante el entrenamiento\n",
    "class ImageDisplayCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        idx = np.random.randint(0, len(X_val))\n",
    "        img = X_val[idx]\n",
    "        true_mask = y_val[idx]\n",
    "        pred_mask = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Máscara Real\")\n",
    "        plt.imshow(true_mask[:, :, 0], cmap='gray')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Predicción del Modelo\")\n",
    "        plt.imshow(pred_mask[:, :, 0], cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"attention_unet_bestv2.h5\", save_best_only=True, monitor=\"val_loss\", mode=\"min\")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e067b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=4,\n",
    "    callbacks=[checkpoint, early_stopping,ImageDisplayCallback()]\n",
    ")\n",
    "\n",
    "print(\"Entrenamiento finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c9821",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError  # O usa la métrica exacta que usaste\n",
    "\n",
    "# Cargar el modelo con la métrica personalizada registrada\n",
    "modelo_cargado = load_model('attention_unet_bestv2.h5')#, custom_objects={'mse': MeanSquaredError()})\n",
    "\n",
    "# Verificar la arquitectura del modelo cargado\n",
    "modelo_cargado.summary()\n",
    "\n",
    "Predict_mask = modelo_cargado.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7365ea",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Función para graficar curvas de entrenamiento y validación\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"\n",
    "    Muestra las curvas de métricas de entrenamiento y validación a lo largo de las épocas.\n",
    "\n",
    "    Parámetros:\n",
    "    - history: Objeto History de Keras que contiene los valores de las métricas por época.\n",
    "    \"\"\"\n",
    "    metrics_to_plot = ['loss', 'accuracy', 'iou_od', 'iou_fovea', 'background_distance_metric']\n",
    "    \n",
    "    # Configurar la figura\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, metric in enumerate(metrics_to_plot, 1):\n",
    "        # Verificar si la métrica existe en el historial\n",
    "        if metric in history.history:\n",
    "            plt.subplot(2, 3, i)\n",
    "            plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "            plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
    "            plt.xlabel('Época')\n",
    "            plt.ylabel(metric)\n",
    "            plt.title(f'{metric.capitalize()} a lo largo de las épocas')\n",
    "            plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Genera y muestra una matriz de confusión basada en valores de colores (negro, gris y blanco).\n",
    "\n",
    "    Parámetros:\n",
    "    - y_true: Máscara real (ground truth).\n",
    "    - y_pred: Máscara predicha por el modelo.\n",
    "    \"\"\"\n",
    "    # Aplanar las matrices de entrada\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "\n",
    "    # Definir los límites para negro, gris y blanco\n",
    "    bins = [0, 0.3, 0.7, 1.01]  # Se extiende el último límite ligeramente para incluir el valor 1.0\n",
    "    # Asignar cada valor a una categoría (0: negro, 1: gris, 2: blanco)\n",
    "    y_true_binned = np.digitize(y_true_flat, bins) - 1\n",
    "    y_pred_binned = np.digitize(y_pred_flat, bins) - 1\n",
    "\n",
    "    # Etiquetas para los colores específicos\n",
    "    labels = [\"Negro (0-0.3)\", \"Gris (0.3-0.7)\", \"Blanco (0.7-1.0)\"]\n",
    "    cm = confusion_matrix(y_true_binned, y_pred_binned, labels=[0, 1, 2])\n",
    "\n",
    "    # Mostrar la matriz de confusión con etiquetas de colores\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title('Matriz de Confusión (Colores)')\n",
    "    plt.show()\n",
    "\n",
    "# Llamado de las funciones de visualización\n",
    "print(\"Visualización de métricas y resultados...\")\n",
    "plot_training_curves(history)\n",
    "\n",
    "# Realizar predicción en el conjunto de validación\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Mostrar matriz de confusión basada en la predicción\n",
    "plot_confusion_matrix(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7099410",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_single_predicted_mask(mask_batch, image_index):\n",
    "    \"\"\"\n",
    "    Grafica una máscara específica de un lote dado su índice.\n",
    "    \n",
    "    Parámetros:\n",
    "    - mask_batch (np.array): Lote de máscaras predichas por el modelo, \n",
    "      con forma (n_imágenes, alto, ancho, canales).\n",
    "    - image_index (int): Índice de la máscara que se desea graficar.\n",
    "    \n",
    "    Consideraciones:\n",
    "    - Se asume que las máscaras están en escala de grises (1 canal).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar que el índice esté dentro del rango del lote de máscaras\n",
    "    if image_index >= mask_batch.shape[0] or image_index < 0:\n",
    "        print(f\"Índice {image_index} fuera de rango. El rango válido es de 0 a {mask_batch.shape[0]-1}.\")\n",
    "        return\n",
    "    \n",
    "    # Seleccionar la máscara correspondiente al índice\n",
    "    mask = mask_batch[image_index, :, :, 0]  # Se selecciona el único canal de la máscara\n",
    "\n",
    "    # Configurar y mostrar el gráfico\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(mask, cmap='gray')  # Mostrar en escala de grises\n",
    "    plt.axis('off')  # Ocultar los ejes para una visualización más limpia\n",
    "    plt.title(f'Máscara en el índice {image_index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a579fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def detect_centers_by_closest_pixels(mask_batch, num_pixels=314):\n",
    "    \"\"\"\n",
    "    Detecta los centros de la fóvea y del disco óptico en un lote de máscaras predichas,\n",
    "    basándose en la proximidad de los píxeles a 0 (fóvea) y a 1 (disco óptico).\n",
    "\n",
    "    Parámetros:\n",
    "    - mask_batch (np.array): Lote de máscaras predichas con forma (N, 224, 224, 1).\n",
    "    - num_pixels (int): Número de píxeles a considerar para el cálculo del centroide (por defecto, 314).\n",
    "\n",
    "    Retorna:\n",
    "    - np.array: Arreglo de coordenadas con forma (N, 2, 2). Para cada imagen se devuelve:\n",
    "                [centro_disco_óptico, centro_fóvea].\n",
    "    \"\"\"\n",
    "    \n",
    "    centers = []\n",
    "    \n",
    "    for mask in mask_batch:\n",
    "        # Asegurar que la máscara tiene una sola dimensión (224, 224)\n",
    "        mask = mask.squeeze()  # Elimina la dimensión extra si es (224, 224, 1)\n",
    "        \n",
    "        # Aplanar la imagen para facilitar la selección de píxeles\n",
    "        flattened_mask = mask.flatten()\n",
    "        \n",
    "        # Calcular la distancia de cada píxel al valor de la fóvea (0) y al del disco óptico (1)\n",
    "        distances_to_0 = np.abs(flattened_mask - 0)  # Distancia a la fóvea\n",
    "        distances_to_1 = np.abs(flattened_mask - 1)  # Distancia al disco óptico\n",
    "        \n",
    "        # Seleccionar los índices de los 314 píxeles más cercanos a 0 (fóvea)\n",
    "        fovea_indices = np.argsort(distances_to_0)[:num_pixels]\n",
    "        \n",
    "        # Seleccionar los índices de los 314 píxeles más cercanos a 1 (disco óptico)\n",
    "        od_indices = np.argsort(distances_to_1)[:num_pixels]\n",
    "        \n",
    "        # Convertir los índices a coordenadas bidimensionales (fila, columna)\n",
    "        fovea_coordinates = np.unravel_index(fovea_indices, mask.shape)\n",
    "        od_coordinates = np.unravel_index(od_indices, mask.shape)\n",
    "        \n",
    "        # Calcular el centroide de las coordenadas más cercanas a la fóvea (si hay valores disponibles)\n",
    "        fovea_center = np.mean(fovea_coordinates, axis=1) if len(fovea_coordinates[0]) > 0 else [None, None]\n",
    "        \n",
    "        # Calcular el centroide de las coordenadas más cercanas al disco óptico\n",
    "        od_center = np.mean(od_coordinates, axis=1) if len(od_coordinates[0]) > 0 else [None, None]\n",
    "        \n",
    "        # Guardar los resultados\n",
    "        centers.append([od_center, fovea_center])\n",
    "    \n",
    "    return np.array(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1440b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "Pred_coordinates = detect_centers_by_closest_pixels(Predict_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f31fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mask_and_centers(mask_batch, centers_batch, index):\n",
    "    \"\"\"\n",
    "    Grafica una máscara predicha junto con los centros del disco óptico y de la fóvea.\n",
    "\n",
    "    Parámetros:\n",
    "    - mask_batch (np.array): Lote de máscaras predichas con forma (N, 224, 224, 1).\n",
    "    - centers_batch (np.array): Lote de coordenadas de centros con forma (N, 2, 2), \n",
    "                                donde cada imagen tiene el centro del disco óptico y de la fóvea.\n",
    "    - index (int): Índice de la imagen que se desea graficar.\n",
    "\n",
    "    Retorno:\n",
    "    - No retorna valores. Muestra una gráfica de la máscara con los centros marcados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener la máscara correspondiente al índice dado\n",
    "    mask = mask_batch[index].squeeze()  # Eliminar la dimensión adicional (224, 224)\n",
    "    \n",
    "    # Obtener las coordenadas de los centros del disco óptico y la fóvea\n",
    "    od_center = centers_batch[index, 0]  # Centro del disco óptico\n",
    "    fovea_center = centers_batch[index, 1]  # Centro de la fóvea\n",
    "    \n",
    "    # Graficar la máscara\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    \n",
    "    # Graficar los centros en la máscara\n",
    "    plt.scatter(od_center[1], od_center[0], color='green', label='Centro del Disco Óptico', \n",
    "                s=100, edgecolor='black')  # Marcador verde para el disco óptico\n",
    "    plt.scatter(fovea_center[1], fovea_center[0], color='red', label='Centro de la Fóvea', \n",
    "                s=100, edgecolor='black')  # Marcador rojo para la fóvea\n",
    "    \n",
    "    # Mostrar leyenda y título\n",
    "    plt.legend()\n",
    "    plt.title(f'Máscara y Centros para la Imagen {index}')\n",
    "    \n",
    "    # Mostrar la gráfica\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec0497",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Carga del modelo y presentación de resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c89e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Librerías\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09041bf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Directorio de imágenes y CSVs\n",
    "image_dir = \"./Rets\"\n",
    "fovea_csv_path = \"./IDRiD_Fovea_Center.csv\"\n",
    "od_csv_path = \"./IDRiD_OD_Center.csv\"\n",
    "output_mask_dir = \"./Masks\"\n",
    "os.makedirs(output_mask_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427ae43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "fovea_data = pd.read_csv(fovea_csv_path)\n",
    "od_data = pd.read_csv(od_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e596d3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Combinar las coordenadas en un DataFrame\n",
    "coordinates_df = pd.merge(fovea_data, od_data, on=\"Image No\", suffixes=(\"_fovea\", \"_od\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec444c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Cargar imágenes y máscaras\n",
    "image_filenames = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "mask_filenames = sorted([f for f in os.listdir(output_mask_dir) if f.endswith(\".png\")])\n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3efbbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "print(\"Cargando imágenes y máscaras...\")\n",
    "for img_file, mask_file in tqdm(zip(image_filenames, mask_filenames), total=len(image_filenames)):\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    mask_path = os.path.join(output_mask_dir, mask_file)\n",
    "\n",
    "    # Leer imagen y máscara\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, image_size)  # Redimensionar para que quepa en memoria\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, image_size)\n",
    "    mask = np.expand_dims(mask, axis=-1) / 255.0  # Normalizar la máscara\n",
    "\n",
    "    X.append(img)\n",
    "    y.append(mask)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd00b5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError  # O usa la métrica exacta que usaste\n",
    "\n",
    "# Cargar el modelo con la métrica personalizada registrada\n",
    "modelo_cargado = load_model('attention_unet_bestv2.h5')#, custom_objects={'mse': MeanSquaredError()})\n",
    "\n",
    "# Verificar la arquitectura del modelo cargado\n",
    "modelo_cargado.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d53f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "````Python\n",
    "\n",
    "# División en conjunto de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba0da3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Buscar los índices\n",
    "indices = [np.where(np.all(X == img, axis=(1, 2, 3)))[0][0] for img in X_val]\n",
    "\n",
    "print(\"Índices del sub-conjunto en el arreglo principal:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961e1a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "#Coordenadas del conjunto de validación.\n",
    "\n",
    "Coordinates_val = coordinates_df.loc[indices].reset_index(drop=True)\n",
    "Coordinates_val\n",
    "\n",
    "Predict_Mask = modelo_cargado.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f810998",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e064af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Grafica una matriz de confusión para evaluar la correspondencia entre las máscaras verdaderas y las predichas,\n",
    "    clasificando los valores en tres categorías: negro, gris y blanco.\n",
    "\n",
    "    Parámetros:\n",
    "    - y_true (np.array): Máscara de valores verdaderos (ground truth).\n",
    "    - y_pred (np.array): Máscara de valores predichos por el modelo.\n",
    "\n",
    "    Funcionalidad:\n",
    "    - Los valores de las máscaras se redondean y agrupan en tres categorías:\n",
    "        - Negro: valores entre 0 y 0.3\n",
    "        - Gris: valores entre 0.3 y 0.7\n",
    "        - Blanco: valores entre 0.7 y 1.0\n",
    "    - Calcula una matriz de confusión que muestra la relación entre las categorías predichas y las verdaderas.\n",
    "    - Visualiza la matriz de confusión con colores para facilitar la interpretación.\n",
    "    \n",
    "    Retorno:\n",
    "    - No retorna valores. Muestra una gráfica de la matriz de confusión.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aplanar las máscaras para facilitar el procesamiento\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "\n",
    "    # Definir los límites para las categorías: negro, gris y blanco\n",
    "    bins = [0, 0.3, 0.7, 1.01]  # Extiende ligeramente el último límite para incluir el 1.0\n",
    "    y_true_binned = np.digitize(y_true_flat, bins) - 1  # Asignar cada valor a una categoría\n",
    "    y_pred_binned = np.digitize(y_pred_flat, bins) - 1\n",
    "\n",
    "    # Mapear las categorías a etiquetas específicas\n",
    "    labels = [\"Negro (0-0.3)\", \"Gris (0.3-0.7)\", \"Blanco (0.7-1.0)\"]\n",
    "\n",
    "    # Calcular la matriz de confusión\n",
    "    cm = confusion_matrix(y_true_binned, y_pred_binned, labels=[0, 1, 2])\n",
    "\n",
    "    # Mostrar la matriz de confusión\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d')  # Colores azules para la visualización\n",
    "    plt.title('Matriz de Confusión (Colores)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cf009",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def detect_centers_by_closest_pixels(mask_batch, num_pixels=314):\n",
    "    \"\"\"\n",
    "    Detectar los centros de la fóvea y del disco óptico en un lote de imágenes de máscaras predichas,\n",
    "    basándose en la proximidad de los píxeles a 0 y 1.\n",
    "\n",
    "    Parameters:\n",
    "        mask_batch (np.array): Lote de máscaras predichas con forma (N, 224, 224, 1).\n",
    "        num_pixels (int): Número de píxeles a considerar para el cálculo del centroide (314 en este caso).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Arreglo de coordenadas, con forma (N, 2, 2), donde para cada imagen se devuelve \n",
    "                  el centro de la fóvea y el centro del disco óptico.\n",
    "    \"\"\"\n",
    "    centers = []\n",
    "    \n",
    "    for mask in mask_batch:\n",
    "        # Asegurarnos de que la máscara está en escala de grises (un solo canal)\n",
    "        mask = mask.squeeze()  # Eliminar la dimensión adicional (224, 224)\n",
    "        \n",
    "        # Aplanar la imagen para facilitar la selección de píxeles\n",
    "        flattened_mask = mask.flatten()\n",
    "        \n",
    "        # Calcular la distancia de cada píxel a 0 (fóvea) y a 1 (disco óptico)\n",
    "        distances_to_0 = np.abs(flattened_mask - 0)  # Distancia a 0 (fóvea)\n",
    "        distances_to_1 = np.abs(flattened_mask - 1)  # Distancia a 1 (disco óptico)\n",
    "        \n",
    "        # Obtener los índices de los 314 píxeles más cercanos a 0\n",
    "        fovea_indices = np.argsort(distances_to_0)[:num_pixels]\n",
    "        # Obtener los índices de los 314 píxeles más cercanos a 1\n",
    "        od_indices = np.argsort(distances_to_1)[:num_pixels]\n",
    "        \n",
    "        # Convertir los índices a coordenadas (fila, columna)\n",
    "        fovea_coordinates = np.unravel_index(fovea_indices, mask.shape)\n",
    "        od_coordinates = np.unravel_index(od_indices, mask.shape)\n",
    "        \n",
    "        # Calcular el centroide de las coordenadas de los píxeles más cercanos a 0 (fóvea)\n",
    "        fovea_center = np.mean(fovea_coordinates, axis=1) if len(fovea_coordinates[0]) > 0 else [None, None]\n",
    "        \n",
    "        # Calcular el centroide de las coordenadas de los píxeles más cercanos a 1 (disco óptico)\n",
    "        od_center = np.mean(od_coordinates, axis=1) if len(od_coordinates[0]) > 0 else [None, None]\n",
    "        \n",
    "        # Guardar los resultados\n",
    "        centers.append([od_center, fovea_center])\n",
    "    \n",
    "    return np.array(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590af6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "Pred_coordinates = detect_centers_by_closest_pixels(Predict_Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb6de2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mask_and_centers(mask_batch, centers_batch, index):\n",
    "    \"\"\"\n",
    "    Grafica la máscara predicha correspondiente al índice dado, junto con los puntos rojos\n",
    "    que representan los centros del disco óptico y de la fóvea.\n",
    "\n",
    "    Parameters:\n",
    "        mask_batch (np.array): Lote de máscaras predichas de forma (N, 224, 224, 1).\n",
    "        centers_batch (np.array): Lote de coordenadas de centros de forma (N, 2, 2),\n",
    "                                  donde para cada imagen se devuelve el centro del disco\n",
    "                                  óptico y de la fóvea.\n",
    "        index (int): Índice de la imagen que se desea graficar.\n",
    "    \"\"\"\n",
    "    # Obtener la máscara correspondiente al índice\n",
    "    mask = mask_batch[index].squeeze()  # Eliminar la dimensión adicional (224, 224)\n",
    "    \n",
    "    # Obtener las coordenadas de los centros del disco óptico y la fóvea\n",
    "    od_center = centers_batch[index, 0]  # Centro del disco óptico\n",
    "    fovea_center = centers_batch[index, 1]  # Centro de la fóvea\n",
    "    \n",
    "    # Graficar la máscara\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    \n",
    "    # Graficar los centros\n",
    "    plt.scatter(od_center[1], od_center[0], color='green', label='Centro del Disco Óptico', s=100, edgecolor='black')\n",
    "    plt.scatter(fovea_center[1], fovea_center[0], color='red', label='Centro de la Fóvea', s=100, edgecolor='black')\n",
    "    \n",
    "    # Mostrar leyenda y título\n",
    "    plt.legend()\n",
    "    plt.title(f'Máscara y Centros para la Imagen {index}')\n",
    "    \n",
    "    # Mostrar la gráfica\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b476b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "def Resize(array, old_size, new_size):\n",
    "    \"\"\"\n",
    "    Ajusta las coordenadas de un conjunto de puntos para un nuevo tamaño de imagen,\n",
    "    escalando las posiciones y reordenando las coordenadas de (y, x) a (x, y).\n",
    "\n",
    "    Parámetros:\n",
    "    - array (np.array): Arreglo de puntos con forma (N, M, 2), donde cada punto \n",
    "                        tiene coordenadas (y, x).\n",
    "    - old_size (tuple): Tamaño anterior de la imagen (alto, ancho).\n",
    "    - new_size (tuple): Tamaño nuevo de la imagen (alto, ancho).\n",
    "\n",
    "    Retorno:\n",
    "    - np.array: Arreglo de puntos ajustados a la nueva escala, con coordenadas (x, y).\n",
    "\n",
    "    Descripción de la funcionalidad:\n",
    "    1. Reordena las coordenadas de cada punto para que sean (x, y) en lugar de (y, x).\n",
    "    2. Escala las coordenadas según la proporción entre `new_size` y `old_size`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reordenar las coordenadas de (y, x) a (x, y)\n",
    "    New_order = np.array([array[i][:, [1, 0]] for i in range(array.shape[0])])\n",
    "\n",
    "    # Escalar las coordenadas según la relación de tamaño nuevo y antiguo\n",
    "    New_size = np.array([\n",
    "        New_order[i] * [new_size[0] / old_size[0], new_size[1] / old_size[1]] \n",
    "        for i in range(New_order.shape[0])\n",
    "    ])\n",
    "    \n",
    "    return New_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa6c73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Redimensionar y ajustar las coordenadas predichas al tamaño original de la imagen\n",
    "Pred_coordinates_new_size = Resize(Pred_coordinates, (224, 224), (4288, 2848))\n",
    "\n",
    "# Crear un DataFrame con las coordenadas ajustadas de la fóvea y el disco óptico\n",
    "Pred_coordinates_df = pd.DataFrame({\n",
    "    \"X-Coordinate_fovea_pred\": Pred_coordinates_new_size[:, 1, 0],  # Coordenadas X de la fóvea\n",
    "    \"Y-Coordinate_fovea_pred\": Pred_coordinates_new_size[:, 1, 1],  # Coordenadas Y de la fóvea\n",
    "    \"X-Coordinate_od_pred\": Pred_coordinates_new_size[:, 0, 0],     # Coordenadas X del disco óptico\n",
    "    \"Y-Coordinate_od_pred\": Pred_coordinates_new_size[:, 0, 1]      # Coordenadas Y del disco óptico\n",
    "})\n",
    "\n",
    "# Combinar las coordenadas verdaderas y predichas en un DataFrame intercalado\n",
    "combined_DF = pd.DataFrame({\n",
    "    'Image': Coordinates_val.iloc[:, 0],         # Nombres o identificadores de las imágenes\n",
    "    'X_fovea': Coordinates_val.iloc[:, 1],       # Coordenada X verdadera de la fóvea\n",
    "    'X_fovea_pred': Pred_coordinates_df.iloc[:, 0],  # Coordenada X predicha de la fóvea\n",
    "    'Y_fovea': Coordinates_val.iloc[:, 2],       # Coordenada Y verdadera de la fóvea\n",
    "    'Y_fovea_pred': Pred_coordinates_df.iloc[:, 1],  # Coordenada Y predicha de la fóvea\n",
    "    'X_od': Coordinates_val.iloc[:, 3],          # Coordenada X verdadera del disco óptico\n",
    "    'X_od_pred': Pred_coordinates_df.iloc[:, 2], # Coordenada X predicha del disco óptico\n",
    "    'Y_od': Coordinates_val.iloc[:, 4],          # Coordenada Y verdadera del disco óptico\n",
    "    'Y_od_pred': Pred_coordinates_df.iloc[:, 3], # Coordenada Y predicha del disco óptico\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030383be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "#Ejecución del código anterior:\n",
    "\n",
    "x_real = np.array(combined_DF[\"X_od\"])\n",
    "x_pred = np.array(combined_DF[\"X_od_pred\"])\n",
    "y_real = np.array(combined_DF[\"Y_od\"])\n",
    "y_pred = np.array(combined_DF[\"Y_od_pred\"])\n",
    "\n",
    "RMSE_od, MAE_od = errors(x_real,y_real,x_pred,x_pred)\n",
    "\n",
    "print('RMSE_od: {}\\nMAE_od: {}'.format(RMSE_od,MAE_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5babe1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "def errors(x_real, y_real, x_pred, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula dos tipos diferentes de errores para evaluar el rendimiento del modelo.\n",
    "    \n",
    "    Actualmente implementa:\n",
    "    1. **Raiz del error cuadrático medio (RMSE)**: mide la diferencia promedio al cuadrado entre los valores reales y predichos.\n",
    "    2. **Error absoluto promedio (MAE)**: mide la diferencia promedio absoluta entre los valores reales y predichos.\n",
    "    \n",
    "    Parameters:\n",
    "        x_real (array-like): Coordenadas X reales.\n",
    "        y_real (array-like): Coordenadas Y reales.\n",
    "        x_pred (array-like): Coordenadas X predichas.\n",
    "        y_pred (array-like): Coordenadas Y predichas.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contiene el RMSE y el MAE.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cálculo del Error Cuadrático Medio (Root Mean Square Error)\n",
    "    RMSE = (((x_real - x_pred)**2 + (y_real - y_pred)**2).sum() / 104) ** 0.5\n",
    "    \n",
    "    # Cálculo del Error Absoluto Promedio (Mean Absolute Error)\n",
    "    MAE = (1 / 104) * (((x_real - x_pred)**2 + (y_real - y_pred)**2) ** 0.5).sum()\n",
    "\n",
    "    return RMSE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc41ff7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image_with_points(image_path, target_size, points):\n",
    "    \"\"\"\n",
    "    Grafica una imagen redimensionada con puntos ajustados correctamente al borde inferior izquierdo.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: ruta de la imagen\n",
    "    - target_size: (ancho, alto) deseado para la imagen\n",
    "    - points: lista de coordenadas (x, y) ya escaladas a la imagen original\n",
    "    \"\"\"\n",
    "    # Cargar imagen RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Redimensionar la imagen al tamaño deseado\n",
    "    target_width, target_height = target_size\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\n",
    "\n",
    "    # Graficar la imagen\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(resized_image)\n",
    "\n",
    "    # Colores y etiquetas para los puntos\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    labels = ['Real_Fovea_Center', 'Pred_Fovea_Center', 'Real_OD_Center', 'Pred_OD_Center']\n",
    "\n",
    "    # Dibujar los puntos con las coordenadas tal cual están\n",
    "    for i, (x, y) in enumerate(points):\n",
    "        # No hacemos ningún ajuste o transformación a las coordenadas\n",
    "        plt.scatter(x, y, c=colors[i], marker='x', s=100, label=labels[i])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f'IDRiD_513.jpg')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60e0c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```Python\n",
    "\n",
    "# Ejemplo de uso\n",
    "image_path = \"Rets/IDRiD_513.jpg\"\n",
    "target_size = (400, 266)  # Dimensiones fijas de la imagen\n",
    "points = [(combined_DF.loc[102][1]*400/4288, combined_DF.loc[102][3]*266/2848),\n",
    "         (combined_DF.loc[102][2]*400/4288, combined_DF.loc[102][4]*266/2848),\n",
    "         (combined_DF.loc[102][5]*400/4288, combined_DF.loc[102][7]*266/2848),\n",
    "         (combined_DF.loc[102][6]*400/4288, combined_DF.loc[102][8]*266/2848)]  # Coordenadas originales\n",
    "\n",
    "plot_image_with_points(image_path, target_size, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aeb254",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Segmentación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c98f1-ab22-45f1-87ab-48c175f2fff4",
   "metadata": {},
   "source": [
    "# Librerías y funciones generales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820247ae-9b9d-4666-8a0d-5192240fca06",
   "metadata": {},
   "source": [
    "```python\n",
    "#Librerías a importar:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f749a85-2d87-4abf-8fb1-4f237f28c890",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- CARGA Y PREPROCESAMIENTO DE DATOS ---\n",
    "def load_images_and_masks(image_dir, mask_dir, img_size, suffix):\n",
    "    \"\"\"\n",
    "    Carga imágenes y máscaras desde directorios específicos, ajustando su tamaño y normalizando los datos.\n",
    "\n",
    "    Parámetros:\n",
    "    - image_dir (str): Ruta al directorio que contiene las imágenes.\n",
    "    - mask_dir (str): Ruta al directorio que contiene las máscaras.\n",
    "    - img_size (tuple): Dimensión objetivo para redimensionar las imágenes y máscaras (ancho, alto).\n",
    "    - suffix (str): Sufijo que se añade al nombre del archivo de las máscaras.\n",
    "\n",
    "    Retorna:\n",
    "    - np.array: Arreglo de imágenes preprocesadas.\n",
    "    - np.array: Arreglo de máscaras binarizadas.\n",
    "    \"\"\"\n",
    "    # Construcción de las rutas para las imágenes y las máscaras\n",
    "    # Las imágenes tienen nombres del tipo \"IDRiD_01.jpg\" hasta \"IDRiD_81.jpg\"\n",
    "    image_paths = [os.path.join(image_dir, f\"IDRiD_{str(i).zfill(2)}.jpg\") for i in range(1, 82)]\n",
    "    # Las máscaras tienen nombres del tipo \"IDRiD_01_suffix.tif\"\n",
    "    mask_paths = [os.path.join(mask_dir, f\"IDRiD_{str(i).zfill(2)}_{suffix}.tif\") for i in range(1, 82)]\n",
    "\n",
    "    images, masks = [], []  # Listas para almacenar las imágenes y las máscaras\n",
    "\n",
    "    # Iteración conjunta sobre las rutas de las imágenes y las máscaras\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        # Cargar y redimensionar la imagen\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        # Cargar la máscara en modo escala de grises\n",
    "        mask = load_img(mask_path, target_size=img_size, color_mode='grayscale')\n",
    "\n",
    "        # Convertir la imagen a un arreglo NumPy y normalizar los valores a [0, 1]\n",
    "        img = img_to_array(img) / 255.0\n",
    "        # Convertir la máscara a un arreglo NumPy y normalizar\n",
    "        mask = img_to_array(mask) / 255.0\n",
    "        # Binarizar la máscara: valores mayores que 0 se convierten a 1\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Almacenar las imágenes y las máscaras en las listas\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Convertir las listas a arreglos NumPy\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bee2d0-f066-4167-9376-64d84a35d053",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- ARQUITECTURA DEL MODELO (Attention U-Net) ---\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Multiply, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bloque de atención\n",
    "def attention_block(x, g, inter_channel):\n",
    "    \"\"\"\n",
    "    Implementa un bloque de atención para enfocar la atención del modelo en características relevantes.\n",
    "\n",
    "    Parámetros:\n",
    "    - x: Tensor de entrada desde la rama de salto del codificador.\n",
    "    - g: Tensor de entrada desde la rama de decodificación (up-sampled).\n",
    "    - inter_channel: Número de canales intermedios.\n",
    "\n",
    "    Retorna:\n",
    "    - out: Tensor modulado por atención.\n",
    "    \"\"\"\n",
    "    # Reducción de dimensiones del tensor x\n",
    "    theta_x = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    # Reducción de dimensiones del tensor g\n",
    "    phi_g = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(g)\n",
    "    # Suma de características\n",
    "    add = Add()([theta_x, phi_g])\n",
    "    add = Activation('relu')(add)\n",
    "    # Proyección para generar el mapa de atención\n",
    "    psi = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(add)\n",
    "    psi = Activation('sigmoid')(psi)\n",
    "    # Modulación del tensor original por el mapa de atención\n",
    "    out = Multiply()([x, psi])\n",
    "    return out\n",
    "\n",
    "# Bloque de codificación\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    \"\"\"\n",
    "    Bloque de codificación con convoluciones, normalización y activación ReLU.\n",
    "\n",
    "    Parámetros:\n",
    "    - input_tensor: Tensor de entrada.\n",
    "    - num_filters: Número de filtros para las capas convolucionales.\n",
    "\n",
    "    Retorna:\n",
    "    - x: Tensor procesado por las capas convolucionales.\n",
    "    - p: Tensor tras la operación de max-pooling.\n",
    "    \"\"\"\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # Reducir la dimensión espacial mediante max-pooling\n",
    "    p = MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "# Bloque de decodificación\n",
    "def decoder_block(input_tensor, skip_features, num_filters):\n",
    "    \"\"\"\n",
    "    Bloque de decodificación que combina características del codificador con las del decodificador.\n",
    "\n",
    "    Parámetros:\n",
    "    - input_tensor: Tensor de entrada desde el nivel anterior del decodificador.\n",
    "    - skip_features: Características de la rama de salto del codificador.\n",
    "    - num_filters: Número de filtros para las capas convolucionales.\n",
    "\n",
    "    Retorna:\n",
    "    - x: Tensor procesado por las capas convolucionales.\n",
    "    \"\"\"\n",
    "    # Aumentar la dimensión espacial\n",
    "    up = UpSampling2D((2, 2))(input_tensor)\n",
    "    # Aplicar atención sobre las características del codificador\n",
    "    attention = attention_block(skip_features, up, num_filters // 2)\n",
    "    # Concatenación de las características del codificador y decodificador\n",
    "    concat = concatenate([up, attention])\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(concat)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Construcción del modelo Attention U-Net\n",
    "def attention_unet(input_shape, output_channels):\n",
    "    \"\"\"\n",
    "    Construcción del modelo Attention U-Net.\n",
    "\n",
    "    Parámetros:\n",
    "    - input_shape: Dimensiones de la entrada (alto, ancho, canales).\n",
    "    - output_channels: Número de canales de salida.\n",
    "\n",
    "    Retorna:\n",
    "    - model: Modelo Attention U-Net.\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Codificador\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    # Bottleneck\n",
    "    b1 = Conv2D(1024, (3, 3), padding='same')(p4)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Activation('relu')(b1)\n",
    "    b1 = Conv2D(1024, (3, 3), padding='same')(b1)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Activation('relu')(b1)\n",
    "\n",
    "    # Decodificador\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Salida\n",
    "    outputs = Conv2D(output_channels, (1, 1), activation='sigmoid')(d4)  # Activación sigmoide para segmentación binaria\n",
    "\n",
    "    # Modelo\n",
    "    model = Model(inputs, outputs, name=\"Attention_U-Net\")\n",
    "    return model\n",
    "\n",
    "# Crear el modelo\n",
    "input_shape = (320, 320, 3)   # Dimensiones de la entrada\n",
    "output_channels = 1           # Número de canales de salida (1 para segmentación binaria)\n",
    "model = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e12839-4032-41ad-9dda-57eb3075a374",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Función de pérdida ponderada\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula una función de pérdida binaria ponderada para manejar el desbalance de clases.\n",
    "    Esta pérdida es útil en problemas donde una clase es mucho más frecuente que la otra.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y_true : Tensor\n",
    "        Valores verdaderos (etiquetas binarias).\n",
    "    y_pred : Tensor\n",
    "        Predicciones del modelo (valores continuos entre 0 y 1).\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    Tensor\n",
    "        Valor de la pérdida ponderada promedio.\n",
    "    \"\"\"\n",
    "    # Asegurar que y_true esté en formato flotante y convertirlo en un vector 1D\n",
    "    y_true_f = K.cast(K.flatten(y_true), 'float32')\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "\n",
    "    # Pesos asignados a las clases (0.1 para la clase 0, 0.9 para la clase 1)\n",
    "    # Estos valores deben ajustarse según el balance de las clases en tu problema\n",
    "    class_weights = [0.1, 0.9]\n",
    "\n",
    "    # Calcular el mapa de pesos: asignar pesos según las clases presentes\n",
    "    # Si el valor es 1, se multiplica por class_weights[1]\n",
    "    # Si el valor es 0, se multiplica por class_weights[0]\n",
    "    weight_map = y_true_f * class_weights[1] + (1 - y_true_f) * class_weights[0]\n",
    "\n",
    "    # Calcular la pérdida binaria entre las predicciones y las etiquetas reales\n",
    "    loss = K.binary_crossentropy(y_true_f, y_pred_f)\n",
    "\n",
    "    # Aplicar el mapa de pesos a la pérdida calculada\n",
    "    weighted_loss = loss * weight_map\n",
    "\n",
    "    # Retornar el promedio de la pérdida ponderada\n",
    "    return K.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fe31f-0529-442f-9f97-b0b0eb9c954a",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generador de datos personalizado para entrenamiento de redes neuronales con Keras.\n",
    "    Soporta la carga de imágenes y máscaras en lotes (batches) y permite aplicar \n",
    "    transformaciones de aumento de datos.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "    - images : array-like\n",
    "        Conjunto de imágenes de entrada.\n",
    "    - masks : array-like\n",
    "        Conjunto de máscaras correspondientes a las imágenes.\n",
    "    - batch_size : int\n",
    "        Tamaño del lote.\n",
    "    - augment : bool\n",
    "        Indica si se aplicarán transformaciones de aumento de datos.\n",
    "    - image_datagen : ImageDataGenerator\n",
    "        Generador para el aumento de imágenes.\n",
    "    - mask_datagen : ImageDataGenerator\n",
    "        Generador para el aumento de máscaras.\n",
    "\n",
    "    Métodos:\n",
    "    --------\n",
    "    - __len__() : Devuelve el número total de lotes.\n",
    "    - __getitem__(index) : Devuelve un lote de imágenes y máscaras en el índice dado.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, masks, batch_size, augment=False):\n",
    "        \"\"\"\n",
    "        Inicializa el generador de datos.\n",
    "\n",
    "        Parámetros:\n",
    "        ----------\n",
    "        - images : array-like\n",
    "            Conjunto de imágenes de entrada.\n",
    "        - masks : array-like\n",
    "            Conjunto de máscaras correspondientes a las imágenes.\n",
    "        - batch_size : int\n",
    "            Tamaño del lote.\n",
    "        - augment : bool, opcional (False por defecto)\n",
    "            Si es True, se aplicarán transformaciones de aumento de datos.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "\n",
    "        # Configuración de aumento de datos para imágenes\n",
    "        self.image_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,  # Rotación aleatoria en un rango de ±20 grados\n",
    "            width_shift_range=0.1,  # Desplazamiento horizontal aleatorio\n",
    "            height_shift_range=0.1,  # Desplazamiento vertical aleatorio\n",
    "            shear_range=0.1,  # Transformación de corte aleatorio\n",
    "            zoom_range=0.1,  # Zoom aleatorio\n",
    "            horizontal_flip=True,  # Volteo horizontal aleatorio\n",
    "            fill_mode='nearest'  # Modo de relleno para píxeles vacíos\n",
    "        )\n",
    "\n",
    "        # Configuración de aumento de datos para máscaras\n",
    "        self.mask_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calcula el número de lotes por época.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        int: Número total de lotes por época.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Obtiene un lote de imágenes y máscaras.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        index : int\n",
    "            Índice del lote.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tuple: Un lote de imágenes y sus máscaras correspondientes.\n",
    "        \"\"\"\n",
    "        # Determinar el rango de datos para el lote actual\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_images = self.images[start:end]\n",
    "        batch_masks = self.masks[start:end]\n",
    "\n",
    "        # Si se requiere aumento de datos\n",
    "        if self.augment:\n",
    "            # Generar una semilla aleatoria para sincronizar las transformaciones de imagen y máscara\n",
    "            seed = np.random.randint(1e6)\n",
    "            \n",
    "            # Aplicar transformaciones de aumento a las imágenes\n",
    "            batch_images = np.array([self.image_datagen.random_transform(img, seed=seed) for img in batch_images])\n",
    "            \n",
    "            # Aplicar las mismas transformaciones a las máscaras\n",
    "            batch_masks = np.array([self.mask_datagen.random_transform(mask, seed=seed) for mask in batch_masks])\n",
    "\n",
    "        return batch_images, batch_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a59bf-1b7c-4c56-acae-ce8b833fd8ee",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VisualizePredictionCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para visualizar las predicciones del modelo durante el entrenamiento.\n",
    "    Al final de cada época, muestra la imagen original, la máscara verdadera y la predicción del modelo.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "    - test_image: numpy array\n",
    "        Imagen de prueba utilizada para generar predicciones.\n",
    "    - test_mask: numpy array\n",
    "        Máscara de prueba correspondiente a la imagen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_image, test_mask):\n",
    "        \"\"\"\n",
    "        Constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        - test_image: numpy array\n",
    "            Imagen de prueba (debe estar preprocesada y lista para la entrada del modelo).\n",
    "        - test_mask: numpy array\n",
    "            Máscara correspondiente a la imagen de prueba.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.test_image = test_image  # Imagen de prueba\n",
    "        self.test_mask = test_mask    # Máscara de prueba\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Método llamado al final de cada época. Genera la predicción del modelo\n",
    "        y muestra la imagen, la máscara verdadera y la predicción.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        - epoch: int\n",
    "            Número de la época actual.\n",
    "        - logs: dict, opcional\n",
    "            Diccionario con métricas del modelo.\n",
    "        \"\"\"\n",
    "        # Realizar la predicción con el modelo entrenado\n",
    "        pred_mask = self.model.predict(self.test_image, verbose=0)\n",
    "        \n",
    "        # Extraer la máscara predicha (asumimos que el tensor tiene forma [1, alto, ancho, canales])\n",
    "        pred_mask = pred_mask[0, :, :, 0]  # Extraer la primera muestra y el canal 0\n",
    "\n",
    "        # Crear una figura con tres subgráficos\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Mostrar la imagen original\n",
    "        axes[0].imshow(self.test_image[0, :, :, 0], cmap=\"gray\")\n",
    "        axes[0].set_title(\"Imagen original\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # Mostrar la máscara verdadera\n",
    "        axes[1].imshow(self.test_mask[0, :, :, 0], cmap=\"gray\")\n",
    "        axes[1].set_title(\"Máscara verdadera\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        # Mostrar la máscara predicha\n",
    "        axes[2].imshow(pred_mask, cmap=\"gray\")\n",
    "        axes[2].set_title(f\"Predicción (Época {epoch + 1})\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        # Ajustar el diseño de la figura y mostrarla\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32e6fb-31bb-4224-8f94-814d50466ea1",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def poly_scheduler_OD(epoch, lr):\n",
    "    \"\"\"\n",
    "    Calcula la tasa de aprendizaje utilizando la estrategia Poly, \n",
    "    que disminuye el learning rate de forma polinómica durante el entrenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - epoch: int\n",
    "        La época actual del entrenamiento.\n",
    "    - lr: float\n",
    "        Tasa de aprendizaje actual (no se usa en el cálculo, pero es obligatorio por la API de Keras).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    - new_lr: float\n",
    "        Nueva tasa de aprendizaje calculada.\n",
    "    \"\"\"\n",
    "    initial_learning_rate = 1e-2  # Tasa de aprendizaje inicial\n",
    "    total_epochs = 80            # Número total de épocas para el entrenamiento\n",
    "    power = 0.9                  # Parámetro que controla la velocidad de disminución\n",
    "\n",
    "    # Fórmula de la estrategia Poly\n",
    "    new_lr = initial_learning_rate * (1 - (epoch / total_epochs)) ** power\n",
    "    return new_lr\n",
    "\n",
    "# Crear el planificador de tasa de aprendizaje usando la estrategia Poly\n",
    "poly_lr_scheduler_OD = LearningRateScheduler(poly_scheduler_OD)\n",
    "\n",
    "\n",
    "class PrintLearningRateCallback_OD(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para imprimir la tasa de aprendizaje actual\n",
    "    al inicio de cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Método llamado al comienzo de cada época. Imprime la tasa de aprendizaje actual.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        - epoch: int\n",
    "            La época actual.\n",
    "        - logs: dict, opcional\n",
    "            Diccionario con información de la época (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        # Obtener la tasa de aprendizaje actual del optimizador\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        print(f\"Epoch {epoch + 1}: Learning rate is {lr:.6f}\")\n",
    "\n",
    "\n",
    "# Crear la instancia del callback para imprimir la tasa de aprendizaje\n",
    "print_lr_callback_OD = PrintLearningRateCallback_OD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797832c-6e77-4b72-8126-c35a80439214",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def poly_scheduler(epoch, lr):\n",
    "    \"\"\"\n",
    "    Calcula la tasa de aprendizaje utilizando la estrategia Poly, \n",
    "    que disminuye la tasa de aprendizaje de forma polinómica a lo largo del entrenamiento.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - epoch: int\n",
    "        La época actual durante el entrenamiento.\n",
    "    - lr: float\n",
    "        Tasa de aprendizaje actual (no se usa en esta implementación, pero es obligatorio por la API de Keras).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    - new_lr: float\n",
    "        Nueva tasa de aprendizaje calculada.\n",
    "    \"\"\"\n",
    "    initial_learning_rate = 1e-2  # Tasa de aprendizaje inicial\n",
    "    total_epochs = 180           # Número total de épocas para el entrenamiento\n",
    "    power = 0.9                  # Parámetro que controla la velocidad de disminución\n",
    "\n",
    "    # Fórmula de la estrategia Poly\n",
    "    new_lr = initial_learning_rate * (1 - (epoch / total_epochs)) ** power\n",
    "    return new_lr\n",
    "\n",
    "# Crear el planificador de tasa de aprendizaje usando la estrategia Poly\n",
    "poly_lr_scheduler = LearningRateScheduler(poly_scheduler)\n",
    "\n",
    "\n",
    "class PrintLearningRateCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para imprimir la tasa de aprendizaje actual\n",
    "    al inicio de cada época durante el entrenamiento.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Método llamado al inicio de cada época para mostrar la tasa de aprendizaje.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        - epoch: int\n",
    "            La época actual.\n",
    "        - logs: dict, opcional\n",
    "            Información adicional sobre la época (no utilizado aquí).\n",
    "        \"\"\"\n",
    "        # Obtener la tasa de aprendizaje actual del optimizador\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        print(f\"Epoch {epoch + 1}: Learning rate is {lr:.6f}\")\n",
    "\n",
    "\n",
    "# Instancia del callback para visualizar la tasa de aprendizaje\n",
    "print_lr_callback = PrintLearningRateCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350281f1-60aa-4ca2-849a-c9ef6cbdc3da",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente de Dice, una métrica común para evaluar el rendimiento de modelos de segmentación.\n",
    "    Esta métrica mide la similitud entre dos conjuntos, siendo 1 el valor ideal (superposición total) y 0 \n",
    "    cuando no hay coincidencia.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - y_true: Tensor\n",
    "        Máscara verdadera del conjunto de datos (ground truth).\n",
    "    - y_pred: Tensor\n",
    "        Máscara predicha por el modelo.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    - float\n",
    "        Valor del coeficiente de Dice.\n",
    "\n",
    "    Nota:\n",
    "    -----\n",
    "    El coeficiente de Dice está definido como:\n",
    "    Dice = (2 * |A ∩ B|) / (|A| + |B|)\n",
    "    Donde A es la máscara verdadera y B es la máscara predicha.\n",
    "    \"\"\"\n",
    "    # Factor suave para evitar división por cero\n",
    "    smooth = 1e-6\n",
    "\n",
    "    # Convertir `y_true` a tipo float32 para garantizar compatibilidad con `y_pred`\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "\n",
    "    # Calcular la intersección entre la máscara verdadera y la predicha\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "\n",
    "    # Calcular la unión de las máscaras\n",
    "    union = K.sum(y_true) + K.sum(y_pred)\n",
    "\n",
    "    # Retornar el coeficiente de Dice\n",
    "    return (2. * intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87566d7-e810-43e4-b220-4f9eab1539b9",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- Métricas de entrenamiento y validación ---\n",
    "def plot_training_metrics(history):\n",
    "    \"\"\"\n",
    "    Grafica las métricas de entrenamiento y validación almacenadas en `history`.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - history: History (objeto de Keras)\n",
    "      Contiene el historial de entrenamiento del modelo, con métricas por época.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # Gráfico de precisión (accuracy)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Precisión Entrenamiento')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Precisión Validación')\n",
    "    plt.title('Precisión de Entrenamiento y Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico del coeficiente Dice\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, history.history['dice_coefficient'], label='Coeficiente Dice Entrenamiento')\n",
    "    plt.plot(epochs, history.history['val_dice_coefficient'], label='Coeficiente Dice Validación')\n",
    "    plt.title('Coeficiente Dice de Entrenamiento y Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Coeficiente Dice')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico de pérdida (loss)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, history.history['loss'], label='Pérdida Entrenamiento')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Pérdida Validación')\n",
    "    plt.title('Pérdida de Entrenamiento y Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Curva ROC ---\n",
    "def plot_roc_curve(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Grafica la curva ROC (Receiver Operating Characteristic) y calcula el AUC.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - y_test: Tensor o arreglo\n",
    "      Valores verdaderos de las clases.\n",
    "    - y_pred: Tensor o arreglo\n",
    "      Valores predichos por el modelo.\n",
    "    \"\"\"\n",
    "    # Calcular tasa de falsos positivos y verdaderos positivos\n",
    "    fpr, tpr, _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Gráfico de la curva ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (Área = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.title('Curva ROC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# --- Matriz de Confusión ---\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Grafica la matriz de confusión.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - y_true: Tensor o arreglo\n",
    "      Valores verdaderos de las clases.\n",
    "    - y_pred: Tensor o arreglo\n",
    "      Valores predichos por el modelo.\n",
    "    \"\"\"\n",
    "    # Umbral para convertir predicciones a valores binarios (0 o 1)\n",
    "    y_pred_thresholded = (y_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Calcular la matriz de confusión\n",
    "    cm = confusion_matrix(y_true.ravel(), y_pred_thresholded.ravel())\n",
    "\n",
    "    # Gráfico de la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Valor Verdadero')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3ea46",
   "metadata": {},
   "source": [
    "# Entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce82a8a-2f5c-4b74-ae0b-438107240812",
   "metadata": {},
   "source": [
    "## Disco Óptico:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ac394-1a0a-404c-a223-67a94488f520",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- PARÁMETROS DEL MODELO --- \n",
    "\n",
    "# Tamaño de las imágenes de entrada\n",
    "IMG_SIZE = (320, 320)\n",
    "# Especifica las dimensiones de las imágenes de entrada en formato (ancho, alto).\n",
    "# Estas dimensiones serán utilizadas durante el preprocesamiento y el entrenamiento del modelo.\n",
    "\n",
    "# Ruta del directorio que contiene las imágenes\n",
    "img_path = \"Rets\"\n",
    "# Carpeta que almacena las imágenes de entrada para el entrenamiento o validación.\n",
    "\n",
    "# Ruta del directorio que contiene las máscaras correspondientes\n",
    "mask_path = \"Masks_OD\"\n",
    "# Carpeta que contiene las máscaras de segmentación asociadas a las imágenes de entrada.\n",
    "# Se asume que las máscaras están alineadas pixel a pixel con las imágenes.\n",
    "\n",
    "# Tamaño del lote (batch size)\n",
    "BATCH_SIZE = 4\n",
    "# Cantidad de imágenes y máscaras que se procesarán simultáneamente en cada iteración del entrenamiento.\n",
    "# Un tamaño de lote pequeño (como 4) es útil si se trabaja con hardware de recursos limitados.\n",
    "\n",
    "# Número total de épocas de entrenamiento\n",
    "EPOCHS = 80\n",
    "# Define cuántas veces el conjunto completo de datos pasará por el modelo durante el entrenamiento.\n",
    "# Un valor de 80 suele ser suficiente para muchos problemas de segmentación, pero podría ajustarse según el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e1427-8c4c-4928-ac24-521788a40412",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Carga las imágenes y sus respectivas máscaras para entrenamiento o evaluación.\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE, 'OD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfa5a1-b97e-4629-bc37-e2fac1ef6361",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- División del conjunto de datos ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primera división: conjunto de entrenamiento y temporal (30% para validación y prueba)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "\n",
    "# Segunda división: conjunto de validación y conjunto de prueba (50% de x_temp para validación y 50% para prueba)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae69de-1923-43b9-a22e-438e14019d43",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- AUMENTO DE DATOS (DATA AUGMENTATION) ---\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,           # Rotación aleatoria de la imagen hasta 20 grados\n",
    "    width_shift_range=0.1,       # Desplazamiento horizontal aleatorio de hasta el 10% del ancho de la imagen\n",
    "    height_shift_range=0.1,      # Desplazamiento vertical aleatorio de hasta el 10% del alto de la imagen\n",
    "    shear_range=0.1,             # Transformación de corte (shear) aleatoria de hasta el 10%\n",
    "    zoom_range=0.1,              # Zoom aleatorio de hasta el 10%\n",
    "    horizontal_flip=True,        # Volteo horizontal aleatorio\n",
    "    fill_mode='nearest'          # Estrategia de relleno para píxeles vacíos resultantes de las transformaciones\n",
    ")\n",
    "\n",
    "# Creación de generadores de datos para imágenes y máscaras\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)  # Generador para imágenes con los argumentos definidos\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)   # Generador para máscaras con las mismas transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c3acf-88ac-4823-9f5c-a6505e7ce7df",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajuste de los generadores para normalización (si es necesario)\n",
    "image_datagen.fit(x_train, augment=True)  # Ajusta el generador de imágenes a los datos de entrenamiento con aumento de datos\n",
    "mask_datagen.fit(y_train, augment=True)   # Ajusta el generador de máscaras a los datos de entrenamiento con aumento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df348014-0edc-41e5-9968-d331c1b14fc2",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear generadores para el entrenamiento del modelo\n",
    "\n",
    "# Fijamos una semilla para garantizar la reproducibilidad de los resultados\n",
    "seed = 42\n",
    "\n",
    "# Generador de imágenes\n",
    "# image_datagen: es un objeto previamente configurado para aplicar transformaciones o aumentos de datos\n",
    "# x_train: conjunto de imágenes de entrenamiento\n",
    "# batch_size: tamaño del lote que se procesará en cada paso\n",
    "# seed: semilla utilizada para sincronizar el generador de imágenes con el de máscaras\n",
    "image_generator = image_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "\n",
    "# Generador de máscaras\n",
    "# mask_datagen: es un objeto similar a image_datagen pero diseñado para las máscaras de segmentación\n",
    "# y_train: conjunto de máscaras de entrenamiento correspondiente a las imágenes en x_train\n",
    "mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "\n",
    "# Combinar generadores de imágenes y máscaras\n",
    "# zip empareja cada lote de imágenes con su lote correspondiente de máscaras\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5936d8-c119-49d0-ba6f-de70c7b38853",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear el modelo de red neuronal para segmentación\n",
    "\n",
    "# Dimensiones de la entrada del modelo (alto, ancho, canales de color)\n",
    "# En este caso, imágenes de 320x320 píxeles con 3 canales de color (RGB)\n",
    "input_shape = (320, 320, 3)   \n",
    "\n",
    "# Número de canales de salida\n",
    "# Para una segmentación binaria (fondo vs. objeto), se utiliza un canal de salida\n",
    "output_channels = 1           \n",
    "\n",
    "# Definición del modelo usando una arquitectura U-Net con bloques de atención\n",
    "# attention_unet: función previamente definida o importada que crea el modelo basado en U-Net\n",
    "model_OD = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "# summary() proporciona información sobre las capas, el número de parámetros entrenables\n",
    "# y las dimensiones de salida de cada capa\n",
    "model_OD.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037599c-ad9a-4194-8e92-c0aec37cc64f",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Creación de generadores personalizados para el entrenamiento y validación del modelo\n",
    "\n",
    "# Generador para los datos de entrenamiento\n",
    "# DataGenerator: clase personalizada previamente definida para generar lotes de datos en tiempo real\n",
    "# x_train: conjunto de imágenes de entrenamiento\n",
    "# y_train: conjunto de máscaras de entrenamiento correspondientes\n",
    "# BATCH_SIZE: tamaño del lote procesado en cada paso\n",
    "# augment=True: habilita el aumento de datos (transformaciones aleatorias) para mejorar la generalización del modelo\n",
    "train_generator = DataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "\n",
    "# Generador para los datos de validación\n",
    "# x_val: conjunto de imágenes de validación\n",
    "# y_val: conjunto de máscaras de validación correspondientes\n",
    "# augment=False: no se aplica aumento de datos durante la validación, ya que los datos deben mantenerse consistentes\n",
    "val_generator = DataGenerator(x_val, y_val, BATCH_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ac6b7-b04e-4461-98ec-92506919a736",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Seleccionar una imagen y su máscara correspondiente del conjunto de validación\n",
    "\n",
    "# test_image contiene una imagen de validación\n",
    "# Seleccionamos solo el primer elemento del conjunto x_val\n",
    "# La dimensión resultante es (1, 320, 320, 3): un lote de una imagen RGB de 320x320 píxeles\n",
    "test_image = x_val[0:1]  \n",
    "\n",
    "# test_mask contiene la máscara correspondiente a test_image\n",
    "# Seleccionamos el primer elemento de y_val, con una dimensión de (1, 320, 320, 1)\n",
    "# La máscara es en escala de grises, generalmente binaria (0 para fondo y 1 para objeto)\n",
    "test_mask = y_val[0:1]  \n",
    "\n",
    "# Crear el callback para visualizar las predicciones durante el entrenamiento\n",
    "# VisualizePredictionCallback: clase personalizada que permite observar las predicciones del modelo\n",
    "# sobre test_image y compararlas con test_mask\n",
    "visualize_callback = VisualizePredictionCallback(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d191b-229c-4a98-b28f-4f24bc632826",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajustar el modelo con el generador corregido\n",
    "\n",
    "# Definir el modelo basado en la arquitectura Attention U-Net\n",
    "# input_shape: dimensiones de la entrada de la red (320x320 píxeles, 3 canales para imágenes RGB)\n",
    "# output_channels: número de canales de salida; 1 para segmentación binaria (fondo vs objeto)\n",
    "input_shape = (320, 320, 3)  \n",
    "output_channels = 1  \n",
    "model_OD = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Compilar el modelo\n",
    "# optimizer='adam': algoritmo de optimización eficiente para el ajuste de pesos\n",
    "# loss=weighted_loss: función de pérdida personalizada ponderada, ideal para manejar clases desbalanceadas\n",
    "# metrics=['accuracy', dice_coefficient]: métricas para evaluar el desempeño; accuracy para clasificación\n",
    "# y dice_coefficient para evaluar la calidad de la segmentación\n",
    "model_OD.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy', dice_coefficient])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history_OD = model_OD.fit(\n",
    "    train_generator,  # generador de datos para el entrenamiento\n",
    "    validation_data=(x_val, y_val),  # datos de validación: imágenes y máscaras correspondientes\n",
    "    epochs=EPOCHS,  # número total de épocas para el entrenamiento\n",
    "    steps_per_epoch=len(train_generator),  # número de pasos por cada época\n",
    "    callbacks=[poly_lr_scheduler_OD, print_lr_callback_OD]  # callbacks personalizados para ajustar el aprendizaje\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f93a7e-0c68-4502-84ed-6482fab49542",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- EVALUACIÓN DEL MODELO ---\n",
    "# Evaluar el modelo con datos de prueba\n",
    "# x_test: conjunto de imágenes de prueba\n",
    "# y_test: conjunto de máscaras correspondientes para la prueba\n",
    "# La función evaluate devuelve una lista con el valor de la función de pérdida y las métricas definidas durante la compilación\n",
    "eval_results = model_OD.evaluate(x_test, y_test)\n",
    "\n",
    "# Mostrar los resultados de la evaluación\n",
    "# eval_results[0]: pérdida (loss) en el conjunto de prueba\n",
    "# eval_results[1]: precisión (accuracy) en el conjunto de prueba\n",
    "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
    "\n",
    "# Guardar el modelo entrenado en un archivo\n",
    "# Se guarda en formato HDF5 (.h5), que permite almacenar tanto la arquitectura como los pesos del modelo\n",
    "model_OD.save(\"unetplusplus_opticDisc_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7e5de-ffac-4078-8787-a7ffc0ce007c",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- GENERAR GRÁFICAS Y EVALUACIONES VISUALES ---\n",
    "\n",
    "# Gráfica de las métricas del entrenamiento\n",
    "# plot_training_metrics: función personalizada para graficar el historial de entrenamiento\n",
    "# Se pueden visualizar métricas como pérdida (loss), precisión (accuracy) y el coeficiente Dice\n",
    "plot_training_metrics(history_OD)\n",
    "\n",
    "# Generar predicciones para el conjunto de prueba\n",
    "# model.predict(x_test): realiza predicciones para cada imagen en x_test\n",
    "# y_pred contendrá las máscaras predichas por el modelo\n",
    "y_pred = model_OD.predict(x_test)\n",
    "\n",
    "# Graficar la curva ROC (Receiver Operating Characteristic)\n",
    "# plot_roc_curve: función personalizada para graficar la curva ROC\n",
    "# Esta curva evalúa el desempeño del modelo considerando la relación entre sensibilidad y especificidad\n",
    "plot_roc_curve(y_test, y_pred)\n",
    "\n",
    "# Graficar la matriz de confusión\n",
    "# plot_confusion_matrix: función personalizada para mostrar la matriz de confusión\n",
    "# Evalúa el número de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaca8ec",
   "metadata": {},
   "source": [
    "A partir de este punto, no se comentará el resto, ya que sigue prácticamente el mismo enfoque aplicado en el disco óptico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bb7e5-6956-4841-858a-7321f559d803",
   "metadata": {},
   "source": [
    "## Exudados Fuertes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568384eb-dd05-4d62-a40d-f148bdc2c917",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- PARÁMETROS ---\n",
    "IMG_SIZE = (400, 400)\n",
    "img_path = \"Rets\"\n",
    "mask_path = \"Masks_EX\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 180\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b3ad9-4c4c-402f-8248-1e2b5e96b26d",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE,'EX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8dc66-ea85-4b03-a4cb-eeda3c25755c",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2a080-d879-49b4-909b-cbedc80ad83f",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c4d06-ec97-469d-9156-c0bdcb5aa7ba",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "image_datagen.fit(x_train, augment=True)\n",
    "mask_datagen.fit(y_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01504e-3a33-4071-93aa-0b43ab98a27b",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "seed = 42\n",
    "image_generator = image_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f22538-4623-48af-ab31-4d572e1e2e23",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear el modelo\n",
    "input_shape = (400, 400, 3)   # Dimensiones de la entrada\n",
    "output_channels = 1           # Tipo de salida (ej. 1 para segmentación binaria)\n",
    "model_EX = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Resumen del modelo\n",
    "model_EX.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154266b-ebce-4baf-8d4f-27012fbb4379",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Usar el nuevo generador\n",
    "train_generator = DataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_generator = DataGenerator(x_val, y_val, BATCH_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6607b-9976-452e-b70b-cd2aa70c8784",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "test_image = x_val[0:1]  \n",
    "test_mask = y_val[0:1]  \n",
    "# Crear el callback\n",
    "visualize_callback = VisualizePredictionCallback(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f170702-13e2-48d4-91a7-c14a70f501ab",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajustar el modelo con el generador corregido\n",
    "\n",
    "# Definir el modelo\n",
    "input_shape = (400, 400, 3)  # Ejemplo de tamaño de imagen y 1 canal (escala de grises)\n",
    "output_channels = 1  # Por ejemplo, para segmentación binaria\n",
    "model_EX = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_EX.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy', dice_coefficient])\n",
    "\n",
    "history_EX = model_EX.fit(\n",
    "    train_generator,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    callbacks=[poly_lr_scheduler,print_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aaa9d6-01ae-4e89-9550-a4527484b893",
   "metadata": {},
   "source": [
    "```Python \n",
    "\n",
    "eval_results = model_EX.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
    "\n",
    "model_EX.save(\"unetplusplus_Exudates_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94debac-366b-4000-80b8-54adb5680815",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "plot_training_metrics(history_EX)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "plot_roc_curve(y_test, y_pred)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9cff9-8d15-423d-880f-2635f549a909",
   "metadata": {},
   "source": [
    "## Exudados Suaves:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e57058-c209-46ec-b64e-c519a7c578d1",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "IMG_SIZE = (400, 400)\n",
    "img_path = \"Rets\"\n",
    "mask_path = \"Masks_SE\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 180\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c6cd3-1706-4d13-8dc8-2281f8d64daa",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE,'SE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df6014-ba8e-4445-8528-bbe8f0b9ba46",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f93c5-7ab1-4eb9-be99-ff9a8ec140ab",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1e800-b42b-4f1c-86ef-5bdbd004d11f",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "image_datagen.fit(x_train, augment=True)\n",
    "mask_datagen.fit(y_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58546d0d-e2a4-41a5-bca5-6242b9cf55af",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "seed = 42\n",
    "image_generator = image_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb3627-5691-4cf2-963f-eca690cdf112",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear el modelo\n",
    "input_shape = (400, 400, 3)   # Dimensiones de la entrada\n",
    "output_channels = 1           # Tipo de salida (ej. 1 para segmentación binaria)\n",
    "model_SE = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Resumen del modelo\n",
    "model_SE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f3bca-2aeb-4d99-939d-d9df1d1346f4",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Usar el nuevo generador\n",
    "train_generator = DataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_generator = DataGenerator(x_val, y_val, BATCH_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6cde9-db7e-40d4-a819-3b96b3358f9c",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "test_image = x_val[0:1]  # Dimensión: (1, 320, 320, 3)\n",
    "test_mask = y_val[0:1]  # Dimensión: (1, 320, 320, 1)\n",
    "# Crear el callback\n",
    "visualize_callback = VisualizePredictionCallback(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983af02-40f7-479f-8e86-599e44a284e6",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajustar el modelo con el generador corregido\n",
    "\n",
    "# Definir el modelo\n",
    "input_shape = (400, 400, 3)  # Ejemplo de tamaño de imagen y 1 canal (escala de grises)\n",
    "output_channels = 1  # Por ejemplo, para segmentación binaria\n",
    "model_SE = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_SE.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy', dice_coefficient])\n",
    "\n",
    "history_SE = model_SE.fit(\n",
    "    train_generator,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    callbacks=[poly_lr_scheduler,print_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db544df-48cc-4f51-bb6b-a1bf88880f44",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "eval_results = model_SE.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
    "\n",
    "\n",
    "model_SE.save(\"unetplusplus_Soft_exudates_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d278c-6c38-4889-8f5f-d245ea0ddc5f",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "plot_training_metrics(history_SE)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "plot_roc_curve(y_test, y_pred)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07656fd4-61d2-4fc7-a1f7-ac5d68aaf430",
   "metadata": {},
   "source": [
    "## Microaneurismas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b192f50-65ae-4a8f-b3da-2bc06f922cff",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "IMG_SIZE = (400, 400)\n",
    "img_path = \"Rets\"\n",
    "mask_path = \"Masks_MA\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 180\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe178d9e-177d-4a0c-9b57-e41d1fb27fa7",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE,'MA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0677a7-c6cd-4197-bcf5-b736a2e840ca",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab384705-d5f9-4627-ab9d-7f7225fd7cb2",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfcda0-9428-41ea-8237-867718e2535b",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "image_datagen.fit(x_train, augment=True)\n",
    "mask_datagen.fit(y_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9c519-ab2a-4c21-bf79-bd8a132b94ee",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "seed = 42\n",
    "image_generator = image_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19773dee-aecb-4254-9a45-38d61c1dc8dd",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear el modelo\n",
    "input_shape = (400, 400, 3)   # Dimensiones de la entrada\n",
    "output_channels = 1           # Tipo de salida (ej. 1 para segmentación binaria)\n",
    "model_MA = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Resumen del modelo\n",
    "model_MA.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6384b6a-3cfa-4132-b8e6-733516027f4b",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Usar el nuevo generador\n",
    "train_generator = DataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_generator = DataGenerator(x_val, y_val, BATCH_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75929543-a4d8-4095-8fde-0037a389f2b3",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "test_image = x_val[0:1]  # Dimensión: (1, 320, 320, 3)\n",
    "test_mask = y_val[0:1]  # Dimensión: (1, 320, 320, 1)\n",
    "# Crear el callback\n",
    "visualize_callback = VisualizePredictionCallback(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36509b79-9e8f-49aa-901f-b29d2567eb86",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajustar el modelo con el generador corregido\n",
    "\n",
    "# Definir el modelo\n",
    "input_shape = (400, 400, 3)  # Ejemplo de tamaño de imagen y 1 canal (escala de grises)\n",
    "output_channels = 1  # Por ejemplo, para segmentación binaria\n",
    "model_MA = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_MA.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy', dice_coefficient])\n",
    "\n",
    "history_MA = model_MA.fit(\n",
    "    train_generator,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    callbacks=[poly_lr_scheduler,print_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d0e70-ee54-491c-82e9-5fc74b67e102",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "eval_results = model_MA.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
    "\n",
    "model_MA.save(\"unetplusplus_microaneurysms_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a0bd-f569-4d0d-8fb6-4d91cbba09a9",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "plot_training_metrics(history_MA)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "plot_roc_curve(y_test, y_pred)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8643d49-289d-41e4-b2fe-f51f56b158f9",
   "metadata": {},
   "source": [
    "## Hemorragias:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce838f07-c77f-4339-a49e-5e5c8a701dc4",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "IMG_SIZE = (400, 400)\n",
    "img_path = \"Rets\"\n",
    "mask_path = \"Masks_HE\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 180\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e1958-22db-47e9-8e27-91762af165a6",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE,'HE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96d39c-9eb0-4d36-b537-f1941358f8d0",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c80830-5362-41a6-a0fc-36b18b8744bf",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232155fa-d561-41dc-af08-7dc6bfd471e5",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "image_datagen.fit(x_train, augment=True)\n",
    "mask_datagen.fit(y_train, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fb473-847b-4164-b932-b3181f5ff499",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "seed = 42\n",
    "image_generator = image_datagen.flow(x_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c09642-6f2b-41ae-9433-36fa3bbded9a",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Crear el modelo\n",
    "input_shape = (400, 400, 3)   # Dimensiones de la entrada\n",
    "output_channels = 1           # Tipo de salida (ej. 1 para segmentación binaria)\n",
    "model_HE = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Resumen del modelo\n",
    "model_HE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054dedb-fcc3-4146-b274-5e758b302829",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Usar el nuevo generador\n",
    "train_generator = DataGenerator(x_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_generator = DataGenerator(x_val, y_val, BATCH_SIZE, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22c6f6-0b18-48e6-a9c3-ef71ad2b78a2",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "test_image = x_val[0:1]  # Dimensión: (1, 320, 320, 3)\n",
    "test_mask = y_val[0:1]  # Dimensión: (1, 320, 320, 1)\n",
    "# Crear el callback\n",
    "visualize_callback = VisualizePredictionCallback(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89db817-c11c-4948-9ff7-f8e830517aec",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Ajustar el modelo con el generador corregido\n",
    "\n",
    "# Definir el modelo\n",
    "input_shape = (400, 400, 3)  # Ejemplo de tamaño de imagen y 1 canal (escala de grises)\n",
    "output_channels = 1  # Por ejemplo, para segmentación binaria\n",
    "model_HE = attention_unet(input_shape, output_channels)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_HE.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy', dice_coefficient])\n",
    "\n",
    "history_HE = model_HE.fit(\n",
    "    train_generator,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    callbacks=[poly_lr_scheduler,print_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365c862-842c-4345-b359-52f3714025e6",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "eval_results = model_MA.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
    "\n",
    "model_MA.save(\"unetplusplus_Hemorrhages_segmentation.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe24f71-4fe9-4462-9cb3-65d3e221bc27",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "plot_training_metrics(history_HE)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "plot_roc_curve(y_test, y_pred)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25c9f7",
   "metadata": {},
   "source": [
    "# Guardar las historias:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818baef-777c-494a-96d1-697541fce56d",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import json\n",
    "\n",
    "# Guardar history_OD como JSON\n",
    "with open('history_OD.json', 'w') as file:\n",
    "    json.dump(history_OD.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d802c-b474-4a5a-ad75-d72e3c80f81e",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import json\n",
    "\n",
    "# Guardar history_EX como JSON\n",
    "with open('history_EX.json', 'w') as file:\n",
    "    json.dump(history_EX.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d7451-fa79-4361-b6bf-f8c0d4ccf3f4",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import json\n",
    "\n",
    "# Guardar history_MA como JSON\n",
    "with open('history_MA.json', 'w') as file:\n",
    "    json.dump(history_MA.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ad0d6-e171-4d26-ae68-3c77ec739393",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import json\n",
    "\n",
    "# Guardar history_SE como JSON\n",
    "with open('history_SE.json', 'w') as file:\n",
    "    json.dump(history_SE.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce503d2f-9050-406c-9a84-5e2ed9c02d0d",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import json\n",
    "\n",
    "# Guardar history_HE como JSON\n",
    "with open('history_HE.json', 'w') as file:\n",
    "    json.dump(history_HE.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59e9dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Carga de los modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81c7d1",
   "metadata": {},
   "source": [
    "## Disco óptico:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642053d",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7a867",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Importar la función para cargar modelos preentrenados de Keras\n",
    "from tensorflow.keras.models import load_model  \n",
    "\n",
    "# Cargar el modelo previamente guardado\n",
    "# 'unetplusplus_opticDisc_segmentation.h5': archivo que contiene el modelo entrenado\n",
    "# compile=False: evita compilar el modelo al cargarlo, útil si solo se requiere para inferencia\n",
    "modelo = load_model('unetplusplus_opticDisc_segmentation.h5', compile=False)\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "# Proporciona información detallada sobre la arquitectura del modelo, el número de capas y parámetros\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679fba5",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- PARÁMETROS ---\n",
    "IMG_SIZE = (320, 320)\n",
    "img_path = \"Rets\"\n",
    "mask_path = \"Masks_OD\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ee8c4",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# --- CARGA Y PREPROCESAMIENTO DE DATOS ---\n",
    "\n",
    "def load_images_and_masks(image_dir, mask_dir, img_size):\n",
    "    \"\"\"\n",
    "    Carga y preprocesa imágenes y sus máscaras correspondientes desde directorios específicos.\n",
    "\n",
    "    Parámetros:\n",
    "    - image_dir: ruta del directorio que contiene las imágenes.\n",
    "    - mask_dir: ruta del directorio que contiene las máscaras.\n",
    "    - img_size: tamaño objetivo al que se redimensionarán las imágenes y máscaras (alto, ancho).\n",
    "\n",
    "    Retorno:\n",
    "    - images: arreglo numpy con las imágenes preprocesadas.\n",
    "    - masks: arreglo numpy con las máscaras binarizadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generar las rutas de las imágenes y sus máscaras\n",
    "    # Los archivos de imágenes siguen el patrón \"IDRiD_XX.jpg\"\n",
    "    # Las máscaras siguen el patrón \"IDRiD_XX_OD.tif\"\n",
    "    image_paths = [os.path.join(image_dir, f\"IDRiD_{str(i).zfill(2)}.jpg\") for i in range(1, 82)]\n",
    "    mask_paths = [os.path.join(mask_dir, f\"IDRiD_{str(i).zfill(2)}_OD.tif\") for i in range(1, 82)]\n",
    "\n",
    "    images, masks = [], []  # Listas para almacenar imágenes y máscaras cargadas\n",
    "\n",
    "    # Iterar sobre pares de imágenes y máscaras\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        # Cargar y redimensionar la imagen\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "\n",
    "        # Cargar y redimensionar la máscara en escala de grises\n",
    "        mask = load_img(mask_path, target_size=img_size, color_mode='grayscale')\n",
    "\n",
    "        # Convertir la imagen y la máscara a arreglos numpy y normalizarlos en el rango [0, 1]\n",
    "        img = img_to_array(img) / 255.0\n",
    "        mask = img_to_array(mask) / 255.0\n",
    "\n",
    "        # Binarizar la máscara: los valores mayores que 0 se convierten en 1 (segmentación binaria)\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Almacenar las imágenes y máscaras preprocesadas\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Convertir listas a arreglos numpy\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e387c",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "images, masks = load_images_and_masks(img_path, mask_path, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b1351",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "# Partir dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9acdc9",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "pred_mask = modelo.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016a2a6",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mostrar_comparacion(imagenes_rgb, mascaras_reales, mascaras_predichas, indice):\n",
    "    \"\"\"\n",
    "    Grafica una retinografía, su máscara real y su máscara predicha de manera contigua.\n",
    "\n",
    "    Parámetros:\n",
    "    - imagenes_rgb: numpy array de imágenes RGB con forma (n, 400, 400, 3)\n",
    "      Conjunto de imágenes originales (retinografías).\n",
    "    - mascaras_reales: numpy array de máscaras reales con forma (n, 400, 400, 1)\n",
    "      Conjunto de máscaras reales asociadas a las imágenes.\n",
    "    - mascaras_predichas: numpy array de máscaras predichas con forma (n, 400, 400, 1)\n",
    "      Conjunto de máscaras generadas por el modelo.\n",
    "    - indice: int, índice de la imagen a mostrar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer la imagen y las máscaras correspondientes\n",
    "    imagen = imagenes_rgb[indice]  # Imagen original RGB\n",
    "    mascara_real = mascaras_reales[indice].squeeze()  # Máscara real ajustada para 2D\n",
    "    mascara_predicha = mascaras_predichas[indice].squeeze()  # Máscara predicha ajustada para 2D\n",
    "\n",
    "    # Configurar la figura para mostrar las tres imágenes\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Mostrar la imagen original\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(imagen)\n",
    "    plt.title(\"Retinografía\")\n",
    "    plt.axis(\"off\")  # Ocultar ejes\n",
    "\n",
    "    # Mostrar la máscara real en escala de grises\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mascara_real, cmap='gray')\n",
    "    plt.title(\"Máscara Real\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Mostrar la máscara predicha en escala de grises\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mascara_predicha, cmap='gray')\n",
    "    plt.title(\"Máscara Predicha\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Ajustar el diseño de la figura y mostrarla\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a6bc0",
   "metadata": {},
   "source": [
    "```Python \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener las puntuaciones del modelo para el conjunto de validación\n",
    "# modelo.predict(x_val): genera las probabilidades predichas para cada pixel (en este caso, la clase positiva)\n",
    "# flatten(): convierte el arreglo en un vector unidimensional\n",
    "y_scores = modelo.predict(x_val).flatten()  \n",
    "\n",
    "# Asegurarse de que las etiquetas de las máscaras también sean unidimensionales\n",
    "y_val = y_val.flatten()  \n",
    "\n",
    "# Calcular la curva Precision-Recall\n",
    "# precision: valores de precisión a diferentes umbrales\n",
    "# recall: valores de recuperación (recall) correspondientes\n",
    "# _: umbrales utilizados para calcular precision y recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_scores)\n",
    "\n",
    "# Graficar la curva Precision-Recall\n",
    "plt.plot(recall, precision)\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b05d83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "El procedimiento para cargar los otros modelos y generar sus respectivas gráficas es idéntico a este."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
