{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nombre:** Andrés Felipe Riaño Quintanilla.\n",
    "### **Cédula:** 1083928808."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerías:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clase:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    '''\n",
    "    Clase diseñada para realizar un proceso de clasificación binaria utilizando una red neuronal, \n",
    "    a partir de datos de entrenamiento y prueba proporcionados.\n",
    "\n",
    "    Parámetros:\n",
    "    ----------\n",
    "    N_epoch : int\n",
    "        Número de épocas para entrenar el algoritmo.\n",
    "    structure : numpy.ndarray\n",
    "        Arreglo con el número de neuronas que tendrá cada capa.\n",
    "    m_train : int\n",
    "        Número de datos de entrenamiento.\n",
    "    m_test : int\n",
    "        Número de datos de prueba.\n",
    "    Xtrain : numpy.ndarray\n",
    "        Arreglo con los datos de entrenamiento. Las filas deben ser las características y las\n",
    "        columnas los datos.\n",
    "    ytrain : numpy.ndarray\n",
    "        Arreglo con las etiquetas de los datos de entrenamiento.\n",
    "    Xtest : numpy.ndarray\n",
    "        Arreglo con los datos de prueba. Las filas deben ser las características y las \n",
    "        columnas los datos.\n",
    "    ytest : numpy.ndarray\n",
    "        Arreglo con las etiquetas de los datos de prueba.\n",
    "    alpha : float\n",
    "        Tasa de aprendizaje del gradiente descendente.\n",
    "\n",
    "    Todos los parámetros se convierten en atributos del objeto.\n",
    "\n",
    "    Otros atributos:\n",
    "    ----------------\n",
    "    w : numpy.ndarray of object\n",
    "        Almacena las matrices de pesos asociadas a cada capa después de haber ejecutado el método\n",
    "        Binary_Classification.\n",
    "\n",
    "    b : numpy.ndarray of object\n",
    "        Almacena los vectores de sesgos asociados a cada capa después de haber ejecutado el método\n",
    "        Binary_Classification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,N_epoch,structure,Xtrain,ytrain,Xtest,ytest,alpha):\n",
    "        '''\n",
    "        Inicializa la clase con los parámetros dados. \n",
    "        '''\n",
    "        self.N_epoch = N_epoch\n",
    "        self.structure = structure\n",
    "        self.m_train = len(ytrain)\n",
    "        self.m_test = len(ytest)\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        self.Xtest = Xtest\n",
    "        self.ytest = ytest\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        '''\n",
    "        Calcula el valor de la función de activación rectificadora evaluada en la entrada x.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        x : float\n",
    "            Valor real a evaluar.\n",
    "        \n",
    "        Retorna:\n",
    "        -------- \n",
    "        float\n",
    "            El valor de entrada si es mayor o igual a 0, o 0 si es negativo.\n",
    "        '''\n",
    "        if x >= 0:\n",
    "        \n",
    "            return x\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            return 0\n",
    "    \n",
    "    def Sigmoid(self, x):\n",
    "        '''\n",
    "        Calcula el valor de la función sigmoide evaluada en la entrada x.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        x : float\n",
    "            Valor real a evaluar.\n",
    "\n",
    "        Retorna :\n",
    "        --------\n",
    "        sigma : float\n",
    "            Valor de la función sigmoide evaluada en x.\n",
    "        '''\n",
    "        sigma = 1/(1 + np.exp(-x))\n",
    "        \n",
    "        return sigma\n",
    "    \n",
    "    def DReLU(self, x):\n",
    "        '''\n",
    "        Calcula el valor de la derivada de la función de activación rectificadora evaluada\n",
    "        en la entrada x.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        x : float\n",
    "            Valor real a evaluar.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        float\n",
    "            1 si el valor de entrada es mayor o igual que cero, o 0 en caso contrario.\n",
    "        '''\n",
    "        if x >= 0:\n",
    "\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "\n",
    "            return 0\n",
    "        \n",
    "    def Cost(self, A_L):\n",
    "        '''\n",
    "        Calcula el valor de la función de coste para los datos de entrenamiento ingresados en\n",
    "        una época determinada.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        A_L : numpy.ndarray\n",
    "            Vector fila con los valores de salida de la red neuronal.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        J : float\n",
    "            Valor de la función de coste evaluada en los datos de entrenamiento y en la salida\n",
    "            de la red neuronal en una época determinada.\n",
    "        '''\n",
    "        J = - (1/self.m_train)*np.sum(self.ytrain*np.log(A_L) + (1 - self.ytrain)*np.log(1 - A_L))\n",
    "\n",
    "        return J\n",
    "        \n",
    "    def Forward(self, g, A_l_1, W_l, b_l):\n",
    "        '''\n",
    "        Dada una función de activación g, la salida A_l_1 de la capa l-1, la matriz de \n",
    "        pesos de la capa l W_l y el vector de sesgos de la capa l b_l, devuelve Z_l y A_l,\n",
    "        que son la entrada de la función de activación y la salida de la capa l,\n",
    "        respectivamente.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        g : callable\n",
    "            Función de activación sin vectorizar.\n",
    "        A_l_1 : numpy.ndarray\n",
    "            Matriz de salida de la capa l-1.\n",
    "        W_l : numpy.ndarray\n",
    "            Matriz de pesos de la capa l.\n",
    "        b_l : numpy.ndarray\n",
    "            Vector de sesgos de la capa l.\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        A_l : numpy.ndarray\n",
    "            Matriz de salida de la capa l.\n",
    "        Z_l : numpy.ndarray\n",
    "            Entrada de la función de activación de la capa l.\n",
    "        '''\n",
    "        Z_l = W_l@A_l_1 + b_l\n",
    "        A_l = np.vectorize(g)(Z_l)\n",
    "\n",
    "        return A_l, Z_l \n",
    "    \n",
    "    def Backward_L(self, delta_L, A_L_1, W_L, b_L):\n",
    "        '''\n",
    "        Efectúa el proceso de retropropagación de la capa de salida de la red neuronal.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        delta_L : numpy.ndarray\n",
    "            Diferencia entre la salida de la red A_L y el vector fila de datos etiquetados y.\n",
    "        A_L_1 : numpy.ndarray\n",
    "            Matriz de salida de la penúltima capa A_L_1.\n",
    "        W_L : numpy.ndarray\n",
    "            Matriz de pesos de la última capa L.\n",
    "        b_L : numpy.ndarray\n",
    "            Vector de sesgos de la última capa L.\n",
    "\n",
    "        Retorna: \n",
    "        --------\n",
    "        W_L : numpy.ndarray\n",
    "            Matriz de pesos de la capa L con la corrección del gradiente descendente.\n",
    "        b_L : numpy.ndarray\n",
    "            Vector de sesgos de la capa L con la corrección del gradiente descendente.\n",
    "        '''\n",
    "        dJdW = ((1/self.m_train)*delta_L) @ (A_L_1.T)\n",
    "        dJdb = (1/self.m_train)*np.sum(delta_L, axis=0)\n",
    "        W_L = W_L - (self.alpha * dJdW)\n",
    "        b_L = b_L - (self.alpha  * dJdb)\n",
    "\n",
    "        return W_L, b_L\n",
    "    \n",
    "    def Backward(self, delta_l1, W_l1, Z_l, A_l_1, W_l, b_l):\n",
    "        '''\n",
    "        Efectúa el proceso de retropropagación en las capas ocultas de la red neuronal.\n",
    "\n",
    "        Parámetros: \n",
    "        -----------\n",
    "        delta_l1 : numpy.ndarray\n",
    "            Denota el producto de Hadamard entre el producto de la transpuesta de la \n",
    "            matriz de pesos en la capa l+2 con el mismo vector delta, pero de la capa\n",
    "            l+2 y la derivada de la función de activación de la capa l+1 evaluada \n",
    "            en la matriz de entrada Z de la función de activación de la capa l+1\n",
    "            (\\delta^{[l+1]}=( W^{[l+2]})^T \\delta^{[l+2]} \\odot g'( Z^{[l+1]})).\n",
    "        W_l1 : numpy.ndarray\n",
    "            Matriz de pesos de la capa l+1.\n",
    "        Z_l : numpy.ndarray\n",
    "            Entrada de la función de activación de la capa l. \n",
    "        A_l_1 : numpy.ndarray\n",
    "            Matriz de salida de la capa l-1.\n",
    "        W_l : numpy.ndarray\n",
    "            Matriz de pesos de la capa l.\n",
    "        b_l : numpy.ndarray\n",
    "            Vector de sesgos de la capa l.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        W_l : numpy.ndarray\n",
    "            Matriz de pesos de la capa l con la corrección del gradiente descendente.\n",
    "        b_l : numpy.ndarray\n",
    "            Vector de sesgos de la capa l con la corrección delg radiente descendente.\n",
    "        delta_l : numpy.ndarray\n",
    "            Denota el producto de Hadamard entre el producto de la transpuesta de la \n",
    "            matriz de pesos en la capa l+1 con el mismo vector delta, pero de la capa\n",
    "            l+1 y la derivada de la función de activación de la capa l evaluada \n",
    "            en la matriz de entrada Z de la función de activación de la capa l\n",
    "            (\\delta^{[l]}=( W^{[l+1]})^T \\delta^{[l+1]} \\odot g'( Z^{[l]})).\n",
    "        '''\n",
    "        delta_l = (W_l1.T @ delta_l1) * np.vectorize(self.DReLU)(Z_l)\n",
    "        dJdW = ((1/self.m_train)*delta_l) @ (A_l_1.T)\n",
    "        dJdb = (1/self.m_train)*np.sum(delta_l, axis=0)\n",
    "        W_l = W_l - (self.alpha * dJdW)\n",
    "        b_l = b_l - (self.alpha  * dJdb)\n",
    "\n",
    "        return W_l, b_l, delta_l\n",
    "    \n",
    "    def Initialization(self):\n",
    "        '''\n",
    "        Inicializa la red neuronal con pesos y sesgos aleatorios y realiza el proceso de \n",
    "        retropropagación una vez.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        W_n : numpy.ndarray of object\n",
    "            Objeto que almacena en cada una de sus entradas a las diferentes matrices de \n",
    "            pesos de cada capa.\n",
    "        b_n : numpy.ndarray of object\n",
    "            Objeto que almacena en cada una de sus entradas a los diferentes vectores de\n",
    "            sesgos de cada capa.\n",
    "        '''\n",
    "        A_0 = self.Xtrain\n",
    "        A = np.empty(len(self.structure) ,dtype = object)\n",
    "        Z = np.empty(len(self.structure) ,dtype = object)\n",
    "        W = np.empty(len(self.structure) ,dtype = object)\n",
    "        b = np.empty(len(self.structure) ,dtype = object)\n",
    "        delta = np.empty(len(self.structure) ,dtype = object)\n",
    "\n",
    "        A[0] = A_0\n",
    "        Z[0] = 0\n",
    "        W[0] = 0\n",
    "        b[0] = 0\n",
    "\n",
    "        for l,nl in enumerate(self.structure[1:-1], start = 1):\n",
    "\n",
    "            nl_1 = np.shape(A[l-1])[0]\n",
    "            W[l] = 1-2*np.random.random((nl,nl_1))\n",
    "            b[l] = np.tile( 1-2*np.random.random((nl)), (self.m_train, 1)).T\n",
    "            A[l],Z[l] = self.Forward(self.ReLU,A[l-1],W[l],b[l])\n",
    "\n",
    "        nL = self.structure[-1]\n",
    "        nL_1 = np.shape(A[-2])[0]\n",
    "        W[-1] = 1-2*np.random.random((nL,nL_1))\n",
    "        b[-1] = np.tile( 1-2*np.random.random((nL)), (self.m_train, 1)).T\n",
    "        A[-1],Z[-1] = self.Forward(self.Sigmoid,A[-2],W[-1],b[-1])\n",
    "\n",
    "        delta[-1] = A[-1] - self.ytrain\n",
    "        W_n = np.empty(len(self.structure) ,dtype = object)\n",
    "        b_n = np.empty(len(self.structure) ,dtype = object)\n",
    "        W_n[0] = 0 \n",
    "        b_n[0] = 0\n",
    "        W_n[-1], b_n[-1] = self.Backward_L(delta[-1],A[-2],W[-1],b[-1])\n",
    "\n",
    "        for l in range(len(self.structure)-2,0,-1):\n",
    "\n",
    "            W_n[l], b_n[l], delta[l] = self.Backward(delta[l+1],W[l+1],Z[l],A[l-1],W[l],b[l])\n",
    "\n",
    "        return W_n, b_n \n",
    "    \n",
    "    def bin(self, x):\n",
    "        '''\n",
    "        Dado un valor de entrada x, devuelve 0 si x es menor a 0.5 y 1 si es mayor a 0.5.\n",
    "\n",
    "        Parámetros:\n",
    "        x : float\n",
    "            Valor real a evaluar.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        float\n",
    "            0 si el valor de entrada es menor que 0.5, o 1 en caso contrario.\n",
    "        '''\n",
    "        return 0 if x < 0.5 else 1\n",
    "    \n",
    "    def Binary_Classification(self):\n",
    "        '''\n",
    "        Imprime el puntaje de entrenamiento, el puntaje de prueba y el valor de la función\n",
    "        de coste en cada época. Además almacena como atributos las matrices de pesos y los vectores \n",
    "        de sesgos de la última época, esto con el fin de guardar la estructura final de la red neuronal\n",
    "        y poder hacer predicciones.\n",
    "        '''\n",
    "        A_0 = self.Xtrain\n",
    "        A_0T = self.Xtest\n",
    "        W, b = self.Initialization()\n",
    "\n",
    "        for epoch in range(1,self.N_epoch+1):\n",
    "\n",
    "            A = np.empty(len(self.structure) ,dtype = object)\n",
    "            Z = np.empty(len(self.structure) ,dtype = object)\n",
    "\n",
    "            A_test = np.empty(len(self.structure) ,dtype = object)\n",
    "            Z_test = np.empty(len(self.structure) ,dtype = object)\n",
    "\n",
    "            A[0] = A_0\n",
    "            Z[0] = 0\n",
    "\n",
    "            A_test[0] = A_0T\n",
    "            Z_test[0] = 0\n",
    "\n",
    "            for l in range(1,len(self.structure)-1):\n",
    "\n",
    "                A[l],Z[l] = self.Forward(self.ReLU,A[l-1],W[l],b[l])\n",
    "                A_test[l],Z_test[l] = self.Forward(self.ReLU,A_test[l-1],W[l],b[l][:,0:self.m_test])\n",
    "\n",
    "            A[-1], Z[-1] = self.Forward(self.Sigmoid,A[-2],W[-1],b[-1])\n",
    "            A_test[-1],Z_test[-1] = self.Forward(self.Sigmoid,A_test[-2],W[-1],b[-1][:,0:self.m_test])\n",
    "            Output_train = np.vectorize(self.bin)(A[-1])\n",
    "            Output_test = np.vectorize(self.bin)(A_test[-1])\n",
    "            \n",
    "            Train_score = np.sum(Output_train == self.ytrain)/len(self.ytrain)\n",
    "            Test_score = np.sum(Output_test == self.ytest)/len(self.ytest)\n",
    "            cost_function = self.Cost(A[-1])\n",
    "\n",
    "            print('Epoch: {}/{}. Train Score: {}. Test Score: {}. Cost function: {}'.format(epoch,self.N_epoch,Train_score,Test_score,cost_function))\n",
    "\n",
    "            delta = np.empty(len(self.structure) ,dtype = object)\n",
    "            delta[-1] = A[-1] - self.ytrain\n",
    "            W_n = np.empty(len(self.structure) ,dtype = object)\n",
    "            b_n = np.empty(len(self.structure) ,dtype = object)\n",
    "            W_n[0] = 0 \n",
    "            b_n[0] = 0\n",
    "            W_n[-1], b_n[-1] = self.Backward_L(delta[-1],A[-2],W[-1],b[-1])\n",
    "                \n",
    "            for l in range(len(self.structure)-2,0,-1):\n",
    "\n",
    "                W_n[l], b_n[l], delta[l] = self.Backward(delta[l+1],W[l+1],Z[l],A[l-1],W[l],b[l])\n",
    "\n",
    "            W = W_n\n",
    "            b = b_n\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "    def Prediction(self, x):\n",
    "        '''\n",
    "        Una vez definida la estructura de la red neuronal al ejecutar el método \n",
    "        Binary_Classification, predice las etiquetas de los datos ingresados x.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        x : numpy.ndarray\n",
    "            Conjunto de datos a clasificar.\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Vector cuyas entradas corresponden a la clasificación de cada uno de los datos ingresados.\n",
    "        '''\n",
    "        mx = np.shape(x)[1]\n",
    "        A = np.empty(len(self.structure) ,dtype = object)\n",
    "        Z = np.empty(len(self.structure) ,dtype = object)\n",
    "        A[0] = x\n",
    "\n",
    "        for l in range(1,len(self.structure)-1):\n",
    "\n",
    "            A[l], Z[l] = self.Forward(self.ReLU,A[l-1],self.W[l],self.b[l][:,0:mx])\n",
    "\n",
    "        A[-1], Z[-1] = self.Forward(self.Sigmoid,A[-2],self.W[-1],self.b[-1][:,0:mx])\n",
    "\n",
    "        return np.vectorize(self.bin)(A[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos de ejecución:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Dataset de fotos de gatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= \"train_catvnoncat.h5\"\n",
    "train_dataset = h5py.File(data_train, \"r\")\n",
    "\n",
    "data_test= \"test_catvnoncat.h5\"\n",
    "test_dataset = h5py.File(data_test, \"r\")\n",
    "\n",
    "xtrain_classes, xtrain, train_label =\\\n",
    "train_dataset[\"list_classes\"],train_dataset[\"train_set_x\"],train_dataset[\"train_set_y\"]\n",
    "\n",
    "test_classes, xtest,test_label =\\\n",
    "test_dataset[\"list_classes\"],test_dataset[\"test_set_x\"],test_dataset[\"test_set_y\"]\n",
    "\n",
    "ytrain = np.array(list(train_label))\n",
    "Xtrain = (np.reshape(xtrain,(209, 64*64*3))/255).T\n",
    "\n",
    "ytest = np.array(list(test_label))\n",
    "Xtest = (np.reshape(xtest,(50, 64*64*3))/255).T\n",
    "\n",
    "structure = np.array([12288,10,10,1])   #4 capas: 1 de entrada, 1 de salida y dos capas ocultas. Las dos capas ocultas tienen 10 neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Neural_Network(100,structure,Xtrain,ytrain,Xtest,ytest,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7107/1768599265.py:135: RuntimeWarning: divide by zero encountered in log\n",
      "  J = - (1/self.m_train)*np.sum(self.ytrain*np.log(A_L) + (1 - self.ytrain)*np.log(1 - A_L))\n",
      "/tmp/ipykernel_7107/1768599265.py:135: RuntimeWarning: invalid value encountered in multiply\n",
      "  J = - (1/self.m_train)*np.sum(self.ytrain*np.log(A_L) + (1 - self.ytrain)*np.log(1 - A_L))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100. Train Score: 0.3444976076555024. Test Score: 0.66. Cost function: nan\n",
      "Epoch: 2/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.631371267862544\n",
      "Epoch: 3/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.6209227314606658\n",
      "Epoch: 4/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.6107146849831707\n",
      "Epoch: 5/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.6007416311841925\n",
      "Epoch: 6/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5909981374554951\n",
      "Epoch: 7/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5814788400321538\n",
      "Epoch: 8/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5721784478331683\n",
      "Epoch: 9/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5630917459424555\n",
      "Epoch: 10/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5542135987373948\n",
      "Epoch: 11/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5455389526736748\n",
      "Epoch: 12/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5370628387365474\n",
      "Epoch: 13/100. Train Score: 0.6555023923444976. Test Score: 0.34. Cost function: 0.5287803745697929\n",
      "Epoch: 14/100. Train Score: 0.8995215311004785. Test Score: 0.34. Cost function: 0.5206867662946943\n",
      "Epoch: 15/100. Train Score: 0.9473684210526315. Test Score: 0.38. Cost function: 0.5127773100321508\n",
      "Epoch: 16/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.5050473931417078\n",
      "Epoch: 17/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.49749249519178107\n",
      "Epoch: 18/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4901081886756846\n",
      "Epoch: 19/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.48289013948828213\n",
      "Epoch: 20/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4758341071781484\n",
      "Epoch: 21/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.46893594499008867\n",
      "Epoch: 22/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.46219159971271645\n",
      "Epoch: 23/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4555971113455548\n",
      "Epoch: 24/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4491486125998108\n",
      "Epoch: 25/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4428423282465896\n",
      "Epoch: 26/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.43667457432587314\n",
      "Epoch: 27/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.43064175722910886\n",
      "Epoch: 28/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4247403726677223\n",
      "Epoch: 29/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4189670045393251\n",
      "Epoch: 30/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.4133183237028132\n",
      "Epoch: 31/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.407791086672968\n",
      "Epoch: 32/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.40238213424458646\n",
      "Epoch: 33/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3970883900555654\n",
      "Epoch: 34/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3919068590977901\n",
      "Epoch: 35/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.38683462618408704\n",
      "Epoch: 36/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.381868854378939\n",
      "Epoch: 37/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.37700678340010907\n",
      "Epoch: 38/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.37224572799777844\n",
      "Epoch: 39/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3675830763172938\n",
      "Epoch: 40/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3630162882511206\n",
      "Epoch: 41/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.35854289378512566\n",
      "Epoch: 42/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.35416049134386307\n",
      "Epoch: 43/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.34986674613910684\n",
      "Epoch: 44/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3456593885254735\n",
      "Epoch: 45/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3415362123665922\n",
      "Epoch: 46/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3374950734149257\n",
      "Epoch: 47/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.33353388770800746\n",
      "Epoch: 48/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.32965062998354927\n",
      "Epoch: 49/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3258433321155799\n",
      "Epoch: 50/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.32211008157350546\n",
      "Epoch: 51/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3184490199057312\n",
      "Epoch: 52/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3148583412492528\n",
      "Epoch: 53/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.31133629086640946\n",
      "Epoch: 54/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.30788116370979796\n",
      "Epoch: 55/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3044913030161644\n",
      "Epoch: 56/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.3011650989299256\n",
      "Epoch: 57/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2979009871568245\n",
      "Epoch: 58/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.29469744764808525\n",
      "Epoch: 59/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.29155300331531037\n",
      "Epoch: 60/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.28846621877625267\n",
      "Epoch: 61/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2854356991314915\n",
      "Epoch: 62/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.28246008877195483\n",
      "Epoch: 63/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2795380702171469\n",
      "Epoch: 64/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2766683629838707\n",
      "Epoch: 65/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.27384972248516953\n",
      "Epoch: 66/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.27108093895915764\n",
      "Epoch: 67/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.26836083642736036\n",
      "Epoch: 68/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2656882716821409\n",
      "Epoch: 69/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2630621333027554\n",
      "Epoch: 70/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2604813406995468\n",
      "Epoch: 71/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2579448431857594\n",
      "Epoch: 72/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.255451619076437\n",
      "Epoch: 73/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2530006748138465\n",
      "Epoch: 74/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.25059104411885824\n",
      "Epoch: 75/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.24822178716769766\n",
      "Epoch: 76/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.24589198979348217\n",
      "Epoch: 77/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2436007627119445\n",
      "Epoch: 78/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.24134724077074585\n",
      "Epoch: 79/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.23913058222177888\n",
      "Epoch: 80/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2369499680158628\n",
      "Epoch: 81/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.23480460111923365\n",
      "Epoch: 82/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2326937058512405\n",
      "Epoch: 83/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.23061652724265988\n",
      "Epoch: 84/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2285723304140505\n",
      "Epoch: 85/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.22656039997357655\n",
      "Epoch: 86/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.22458003943373642\n",
      "Epoch: 87/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.22263057064644373\n",
      "Epoch: 88/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.22071133325591674\n",
      "Epoch: 89/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.21882168416884268\n",
      "Epoch: 90/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2169609970412955\n",
      "Epoch: 91/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.21512866178189413\n",
      "Epoch: 92/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2133240840707035\n",
      "Epoch: 93/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.21154668489338888\n",
      "Epoch: 94/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2097959000901479\n",
      "Epoch: 95/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2080711799189552\n",
      "Epoch: 96/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.20637198863266873\n",
      "Epoch: 97/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.20469780406955537\n",
      "Epoch: 98/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.20304811725680869\n",
      "Epoch: 99/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.2014224320266415\n",
      "Epoch: 100/100. Train Score: 1.0. Test Score: 0.42. Cost function: 0.19982026464454852\n"
     ]
    }
   ],
   "source": [
    "modelo.Binary_Classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El puntaje de entrenamiento es 1, pero el puntaje de prueba es 0.42. Esto evidencia un sobreajuste en el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: [0 0 1 0 0 0 0 1 0 0]\n",
      "Realidad: [1 1 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print('Predicción: {}\\nRealidad: {}'.format(modelo.Prediction(Xtest[:,0:10])[0],ytest[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que el algoritmo no es muy eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Dataset de cáncer de mama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df[\"target\"]):\n",
    "  strat_train_set = df.loc[train_index]\n",
    "  strat_test_set = df.loc[test_index]\n",
    "\n",
    "df_train = strat_train_set   #Datos de entrenamiento.\n",
    "df_test = strat_test_set   #Datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain2 = df_train.drop('target', axis=1).to_numpy().T\n",
    "ytrain2 = df_train['target'].to_numpy()\n",
    "\n",
    "Xtest2 = df_test.drop('target', axis=1).to_numpy().T\n",
    "ytest2 = df_test['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure2 = np.array([30,2,2,1])\n",
    "\n",
    "modelo2 = Neural_Network(100, structure2,Xtrain2,ytrain2,Xtest2,ytest2,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100. Train Score: 0.6263736263736264. Test Score: 0.631578947368421. Cost function: nan\n",
      "Epoch: 2/100. Train Score: 0.4153846153846154. Test Score: 0.37719298245614036. Cost function: inf\n",
      "Epoch: 3/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 4/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 5/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 6/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 7/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 8/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 9/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 10/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 11/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 12/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 13/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 14/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 15/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 16/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 17/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 18/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 19/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 20/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 21/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 22/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 23/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 24/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 25/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 26/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 27/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 28/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 29/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 30/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 31/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 32/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 33/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 34/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 35/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 36/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 37/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 38/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 39/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 40/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 41/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 42/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 43/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 44/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 45/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 46/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 47/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 48/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 49/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 50/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 51/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 52/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 53/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 54/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 55/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 56/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 57/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 58/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 59/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 60/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 61/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 62/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 63/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 64/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 65/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 66/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 67/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 68/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 69/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 70/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 71/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 72/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 73/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 74/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 75/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 76/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 77/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 78/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 79/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 80/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 81/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 82/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 83/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 84/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 85/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 86/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 87/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 88/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 89/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 90/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 91/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 92/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 93/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 94/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 95/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 96/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 97/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 98/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 99/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n",
      "Epoch: 100/100. Train Score: 1.0. Test Score: 0.47368421052631576. Cost function: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7107/1768599265.py:135: RuntimeWarning: divide by zero encountered in log\n",
      "  J = - (1/self.m_train)*np.sum(self.ytrain*np.log(A_L) + (1 - self.ytrain)*np.log(1 - A_L))\n",
      "/tmp/ipykernel_7107/1768599265.py:135: RuntimeWarning: invalid value encountered in multiply\n",
      "  J = - (1/self.m_train)*np.sum(self.ytrain*np.log(A_L) + (1 - self.ytrain)*np.log(1 - A_L))\n",
      "/tmp/ipykernel_7107/1768599265.py:92: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = 1/(1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "modelo2.Binary_Classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar al caso anterior, se tiene un puntaje de entrenamiento muy alto y un puntaje de prueba muy bajo, lo que indica que este modelo también se sobreajusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: [1 0 1 1 1 1 1 1 1 1]\n",
      "Realidad: [0 1 0 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Predicción: {}\\nRealidad: {}'.format(modelo2.Prediction(Xtest2[:,0:10])[0],ytest2[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, a partir de un ejemplo práctico se ve claramente que el modelo no es muy bueno."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
